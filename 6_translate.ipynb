{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Binary for training translation models and decoding from them.\n",
    "\n",
    "See the following papers for more information on neural translation models.\n",
    " * http://arxiv.org/abs/1409.3215\n",
    " * http://arxiv.org/abs/1409.0473\n",
    " * http://arxiv.org/abs/1412.2007\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.translate import data_utils\n",
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "\n",
    "\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\n",
    "                          \"Learning rate decays by this much.\")\n",
    "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\n",
    "                          \"Clip gradients to this norm.\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64,\n",
    "                            \"Batch size to use during training.\")\n",
    "tf.app.flags.DEFINE_integer(\"size\", 1024, \"Size of each model layer.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")\n",
    "tf.app.flags.DEFINE_integer(\"en_vocab_size\", 40000, \"English vocabulary size.\")\n",
    "tf.app.flags.DEFINE_integer(\"fr_vocab_size\", 40000, \"French vocabulary size.\")\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"/tmp\", \"Data directory\")\n",
    "tf.app.flags.DEFINE_string(\"train_dir\", \"/tmp\", \"Training directory.\")\n",
    "tf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,\n",
    "                            \"Limit on the size of training data (0: no limit).\")\n",
    "tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 200,\n",
    "                            \"How many training steps to do per checkpoint.\")\n",
    "tf.app.flags.DEFINE_boolean(\"decode\", False,\n",
    "                            \"Set to True for interactive decoding.\")\n",
    "tf.app.flags.DEFINE_boolean(\"self_test\", False,\n",
    "                            \"Run a self-test if this is set to True.\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See seq2seq_model.Seq2SeqModel for details of how they work.\n",
    "_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character:  \n",
      "Unexpected character: ï\n",
      "0 25 0 0\n",
      "b {  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) # [a-z]\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(size):\n",
    "    encoder_inputs = [np.random.randint(1, vocabulary_size, size) for _ in xrange(batch_size)]\n",
    "    decoder_inputs = [np.zeros(size+1, dtype = np.int32) for _ in xrange(batch_size)]\n",
    "    weights = [np.ones(size+1, dtype = np.float32) for _ in xrange(batch_size)]\n",
    "    for i in xrange(batch_size):\n",
    "        r = random.randint(1, size-1)\n",
    "        encoder_inputs[i][r:] = 0\n",
    "        # Reverse the encoder input sequence, but leave a 0 at index 0 and at least one 0 at the end.\n",
    "        # These are the GO and EOS markers.\n",
    "        decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1]\n",
    "        weights[i][r+1:] = 0.0\n",
    "    return np.transpose(encoder_inputs), np.transpose(decoder_inputs), np.transpose(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bacthes are three matrices, encoder_inputs and decoder_inputs and weights. Weights are all zeroes.\n",
    "\n",
    "encoder_inputs have one entry per column (a fake word) with one letter per row.\n",
    "\n",
    "decoder_inputs (the \"labels\") are encoder_inputs reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_i\n",
      "[[ 4 10  5 19 10  6  1 18 15 13 14 10  9 20 18 20  1  6 17 21]\n",
      " [17 17 10  9 18 12  9  3  2  5  1 10 20  6  5  7  7  1  8  8]\n",
      " [20 14 21 11 20 18 10 12  8  5 13 13  2 16  5  0 19  4  0  8]\n",
      " [17 20 12  0 13 20  4  7  8  0  0 17  3 11  7  0  3  0  0  2]\n",
      " [ 0  1 11  0 13  6 11 12 15  0  0  9 16  2 16  0 12  0  0  7]\n",
      " [ 0 21 17  0 15 10 16  5  3  0  0 17  0 19  3  0  9  0  0  1]\n",
      " [ 0 14  0  0  5  0 21 14  8  0  0 21  0  5 17  0  0  0  0  0]\n",
      " [ 0  0  0  0 19  0  2 15  0  0  0  2  0 20 10  0  0  0  0  0]\n",
      " [ 0  0  0  0 17  0  6  7  0  0  0  0  0  8  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "d_i\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [17 14 17 11 17 10  6  7  8  5 13  2 16  8 10  7  9  4  8  1]\n",
      " [20 21 11  9 19  6  2 15  3  5  1 21  3 20 17 20 12  1 17  7]\n",
      " [17  1 12 19  5 20 21 14 15 13 14 17  2  5  3  0  3  6  0  2]\n",
      " [ 4 20 21  0 15 18 16  5  8  0  0  9 20 19 16  0 19  0  0  8]\n",
      " [ 0 14 10  0 13 12 11 12  8  0  0 17  9  2  7  0  7  0  0  8]\n",
      " [ 0 17  5  0 13  6  4  7  2  0  0 13  0 11  5  0  1  0  0 21]\n",
      " [ 0 10  0  0 20  0 10 12 15  0  0 10  0 16  5  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  9  3  0  0  0 10  0  6 18  0  0  0  0  0]\n",
      " [ 0  0  0  0 10  0  1 18  0  0  0  0  0 20  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "batch_size=20\n",
    "vocabulary_size=22\n",
    "b = get_batch(10)\n",
    "print('e_i')\n",
    "print(b[0])\n",
    "print('d_i')\n",
    "print(b[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19 16  1  5  9 14 19 11  3  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 7 20 17 14 20 21  7  0 14  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  8 14 23 12  4  4 25  6  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2 22  0 15 17  0 24  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 10 13  0 18  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_validation_batch(text, size):\n",
    "    words = text.split()[:batch_size]\n",
    "    encoder_inputs = [np.zeros(size, dtype = np.int32) for _ in xrange(batch_size)]\n",
    "    for i, word in enumerate(words):\n",
    "        l = min(len(word), size - 1)\n",
    "        for j in xrange(l):\n",
    "            encoder_inputs[i][j] = char2id(word[j])\n",
    "    return np.transpose(encoder_inputs)\n",
    "\n",
    "val_encoder_inputs = get_validation_batch(\"the quick brown fox jumps over the lazy dog\", 10)\n",
    "print(val_encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "Model\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # model = seq2seq_model.Seq2SeqModel(source_vocab_size = vocabulary_size,\n",
    "  #                                    target_vocab_size = vocabulary_size,\n",
    "  #                                     buckets = [(10, 11)],\n",
    "  #                                     size = num_nodes,\n",
    "  #                                     num_layers = 1, # one encoding and one decoding LSTM\n",
    "  #                                     max_gradient_norm = 5.0,\n",
    "  #                                     batch_size = batch_size,\n",
    "  #                                     learning_rate = 1.0,\n",
    "  #                                     learning_rate_decay_factor = 0.9,\n",
    "  #                                     use_lstm = True,\n",
    "  #                                     forward_only = False)\n",
    "    \n",
    "def create_model(session, forward_only):\n",
    "  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "  model = seq2seq_model.Seq2SeqModel(\n",
    "      FLAGS.en_vocab_size, FLAGS.fr_vocab_size, _buckets,\n",
    "      FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,\n",
    "      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor,\n",
    "      forward_only=forward_only)\n",
    "  ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n",
    "  if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "  else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  \"\"\"Train a en->fr translation model using WMT data.\"\"\"\n",
    "  # Prepare WMT data.\n",
    "  print(\"Preparing WMT data in %s\" % FLAGS.data_dir)\n",
    "  en_train, fr_train, en_dev, fr_dev, _, _ = data_utils.prepare_wmt_data(\n",
    "      FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    # Create model.\n",
    "    print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))\n",
    "    model = create_model(sess, False)\n",
    "\n",
    "    # Read data into buckets and compute their sizes.\n",
    "    print (\"Reading development and training data (limit: %d).\"\n",
    "           % FLAGS.max_train_data_size)\n",
    "    dev_set = read_data(en_dev, fr_dev)\n",
    "    train_set = read_data(en_train, fr_train, FLAGS.max_train_data_size)\n",
    "    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "\n",
    "    # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "    # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "    # the size if i-th training bucket, as used later.\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    while True:\n",
    "      # Choose a bucket according to data distribution. We pick a random number\n",
    "      # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "      random_number_01 = np.random.random_sample()\n",
    "      bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                       if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "      # Get a batch and make a step.\n",
    "      start_time = time.time()\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          train_set, bucket_id)\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                   target_weights, bucket_id, False)\n",
    "      step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n",
    "      loss += step_loss / FLAGS.steps_per_checkpoint\n",
    "      current_step += 1\n",
    "\n",
    "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "      if current_step % FLAGS.steps_per_checkpoint == 0:\n",
    "        # Print statistics for the previous epoch.\n",
    "        perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "        print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                         step_time, perplexity))\n",
    "        # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "          sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "        # Save checkpoint and zero timer and loss.\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, \"translate.ckpt\")\n",
    "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        # Run evals on development set and print their perplexity.\n",
    "        for bucket_id in xrange(len(_buckets)):\n",
    "          if len(dev_set[bucket_id]) == 0:\n",
    "            print(\"  eval: empty bucket %d\" % (bucket_id))\n",
    "            continue\n",
    "          encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "              dev_set, bucket_id)\n",
    "          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "          eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\n",
    "          print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should study this.\n",
    "\n",
    "def evaluate_model(model, sess, val_encoder_inputs, output_size):\n",
    "    val_decoder_inputs = np.zeros((output_size+1, batch_size,), dtype = np.int32)\n",
    "    val_target_weights = np.zeros((output_size+1, batch_size,), dtype = np.float32)\n",
    "    val_target_weights[0,:] = 1.0\n",
    "    is_finished = np.full((batch_size,), False, dtype = np.bool_)\n",
    "    for i in xrange(output_size):\n",
    "        _, _, output_logits = model.step(sess, val_encoder_inputs, val_decoder_inputs, val_target_weights, bucket_id = 0, forward_only = True)\n",
    "        p = np.argmax(output_logits[i], axis = 1)\n",
    "        is_finished = np.logical_or(is_finished, p == 0)\n",
    "        val_decoder_inputs[i,:] = (1 - is_finished) * p\n",
    "        val_target_weights[i,:] = (1.0 - is_finished) * 1.0\n",
    "    return val_decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode():\n",
    "  with tf.Session() as sess:\n",
    "    # Create model and load parameters.\n",
    "    model = create_model(sess, True)\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "\n",
    "    # Load vocabularies.\n",
    "    en_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.en\" % FLAGS.en_vocab_size)\n",
    "    fr_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.fr\" % FLAGS.fr_vocab_size)\n",
    "    en_vocab, _ = data_utils.initialize_vocabulary(en_vocab_path)\n",
    "    _, rev_fr_vocab = data_utils.initialize_vocabulary(fr_vocab_path)\n",
    "\n",
    "    # Decode from standard input.\n",
    "    sys.stdout.write(\"> \")\n",
    "    sys.stdout.flush()\n",
    "    sentence = sys.stdin.readline()\n",
    "    while sentence:\n",
    "      # Get token-ids for the input sentence.\n",
    "      token_ids = data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence), en_vocab)\n",
    "      # Which bucket does it belong to?\n",
    "      bucket_id = min([b for b in xrange(len(_buckets))\n",
    "                       if _buckets[b][0] > len(token_ids)])\n",
    "      # Get a 1-element batch to feed the sentence to the model.\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          {bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "      # Get output logits for the sentence.\n",
    "      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "      # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "      # If there is an EOS symbol in outputs, cut them at that point.\n",
    "      if data_utils.EOS_ID in outputs:\n",
    "        outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
    "      # Print out French sentence corresponding to outputs.\n",
    "      print(\" \".join([tf.compat.as_str(rev_fr_vocab[output]) for output in outputs]))\n",
    "      print(\"> \", end=\"\")\n",
    "      sys.stdout.flush()\n",
    "      sentence = sys.stdin.readline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def self_test():\n",
    "  \"\"\"Test the translation model.\"\"\"\n",
    "  with tf.Session() as sess:\n",
    "    print(\"Self-test for neural translation model.\")\n",
    "    # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "    model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2,\n",
    "                                       5.0, 32, 0.3, 0.99, num_samples=8)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Fake data set for both the (3, 3) and (6, 6) bucket.\n",
    "    data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "                [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "    for _ in xrange(5):  # Train the fake model for 5 steps.\n",
    "      bucket_id = random.choice([0, 1])\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          data_set, bucket_id)\n",
    "      model.step(sess, encoder_inputs, decoder_inputs, target_weights,\n",
    "                 bucket_id, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
