{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse words\n",
    "=============\n",
    "\n",
    "Based on translate.py and on code from xxxxx.\n",
    "\n",
    "To do\n",
    "-----\n",
    "\n",
    "- clean old code.\n",
    "- remove save/restore model.\n",
    "- do predict.\n",
    "- check loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Binary for training translation models and decoding from them.\n",
    "\n",
    "See the following papers for more information on neural translation models.\n",
    " * http://arxiv.org/abs/1409.3215\n",
    " * http://arxiv.org/abs/1409.0473\n",
    " * http://arxiv.org/abs/1412.2007\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.translate import data_utils\n",
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "\n",
    "\n",
    "learning_rate = 0.5                  # \"Learning rate.\"\n",
    "learning_rate_decay_factor = 0.9     # \"Learning rate decays by this much.\"\n",
    "max_gradient_norm = 5.0              # \"Clip gradients to this norm.\"\n",
    "num_nodes = 128                      # \"Size of each model layer.\") # was 1024 \n",
    "num_layers = 1                       # \"Number of layers in the model.\") # was 3\n",
    "\n",
    "steps_per_checkpoint = 30            # \"How many training steps to do per checkpoint.\")\n",
    "dodecode = False                     #, \"Set to True for interactive decoding.\")\n",
    "\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See seq2seq_model.Seq2SeqModel for details of how they work.\n",
    "# _buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "_buckets = [(10, 11)]\n",
    "\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character:  \n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z]\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(size):\n",
    "    encoder_inputs = [np.random.randint(1, vocabulary_size, size) for _ in xrange(batch_size)]\n",
    "    decoder_inputs = [np.zeros(size+1, dtype = np.int32) for _ in xrange(batch_size)]\n",
    "    weights = [np.ones(size+1, dtype = np.float32) for _ in xrange(batch_size)]\n",
    "    for i in xrange(batch_size):\n",
    "        r = random.randint(1, size-1)\n",
    "        encoder_inputs[i][r:] = 0\n",
    "        # Reverse the encoder input sequence, but leave a 0 at index 0 and at least one 0 at the end.\n",
    "        # These are the GO and EOS markers.\n",
    "        decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1]\n",
    "        weights[i][r+1:] = 0.0\n",
    "    return np.transpose(encoder_inputs), np.transpose(decoder_inputs), np.transpose(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bacthes are three matrices, encoder_inputs and decoder_inputs and weights. Weights are all zeroes.\n",
    "\n",
    "encoder_inputs have one entry per column (a fake word) with one letter per row.\n",
    "\n",
    "decoder_inputs (the \"labels\") are encoder_inputs reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_i\n",
      "[[ 4 24  1 11 11 15 15 26  4 11 11 26 22 21  3  3]\n",
      " [ 6  1 22  3 21  8  7 13  8  4  3  8  8  1  3  2]\n",
      " [ 2  0 18 11  7  0  0 16  8 19  0 25 13  0  0  6]\n",
      " [ 3  0 18  5 25  0  0 21 13 23  0  2  3  0  0  8]\n",
      " [ 8  0 26 12 14  0  0 23  0  6  0  8  8  0  0  4]\n",
      " [ 0  0 24  0  0  0  0  0  0  5  0  9 13  0  0 11]\n",
      " [ 0  0  5  0  0  0  0  0  0  1  0  0 24  0  0  3]\n",
      " [ 0  0  9  0  0  0  0  0  0 12  0  0  6  0  0  0]\n",
      " [ 0  0 22  0  0  0  0  0  0 13  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "d_i\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 8  1 22 12 14  8  7 23 13 13  3  9  6  1  3  3]\n",
      " [ 3 24  9  5 25 15 15 21  8 12 11  8 24 21  3 11]\n",
      " [ 2  0  5 11  7  0  0 16  8  1  0  2 13  0  0  4]\n",
      " [ 6  0 24  3 21  0  0 13  4  5  0 25  8  0  0  8]\n",
      " [ 4  0 26 11 11  0  0 26  0  6  0  8  3  0  0  6]\n",
      " [ 0  0 18  0  0  0  0  0  0 23  0 26 13  0  0  2]\n",
      " [ 0  0 18  0  0  0  0  0  0 19  0  0  8  0  0  3]\n",
      " [ 0  0 22  0  0  0  0  0  0  4  0  0 22  0  0  0]\n",
      " [ 0  0  1  0  0  0  0  0  0 11  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# batch_size=20\n",
    "b = get_batch(10)\n",
    "print('e_i')\n",
    "print(b[0])\n",
    "print('d_i')\n",
    "print(b[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 17  2  6 10 15 20 12  4  0  0  0  0  0  0  0]\n",
      " [ 8 21 18 15 21 22  8  1 15  0  0  0  0  0  0  0]\n",
      " [ 5  9 15 24 13  5  5 26  7  0  0  0  0  0  0  0]\n",
      " [ 0  3 23  0 16 18  0 25  0  0  0  0  0  0  0  0]\n",
      " [ 0 11 14  0 19  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "the quick brown fox jumps over the lazy dog       \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_validation_batch(text, size):\n",
    "    words = text.split()[:batch_size]\n",
    "    encoder_inputs = [np.zeros(size, dtype = np.int32) for _ in xrange(batch_size)]\n",
    "    for i, word in enumerate(words):\n",
    "        l = min(len(word), size - 1)\n",
    "        for j in xrange(l):\n",
    "            encoder_inputs[i][j] = char2id(word[j])\n",
    "    return np.transpose(encoder_inputs)\n",
    "\n",
    "def totext(inputs):\n",
    "    inp = np.transpose(inputs)\n",
    "    toletters = np.vectorize(id2char)\n",
    "    return ' '.join([ ''.join(row).rstrip() for row in toletters(inp)])\n",
    "\n",
    "\n",
    "val_encoder_inputs = get_validation_batch(\"the quick brown fox jumps over the lazy dog\", 10)\n",
    "print(val_encoder_inputs)\n",
    "print(totext(val_encoder_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "Model\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # model = seq2seq_model.Seq2SeqModel(source_vocab_size = vocabulary_size,\n",
    "  #                                    target_vocab_size = vocabulary_size,\n",
    "  #                                     buckets = [(10, 11)],\n",
    "  #                                     size = num_nodes,\n",
    "  #                                     num_layers = 1, # one encoding and one decoding LSTM\n",
    "  #                                     max_gradient_norm = 5.0,\n",
    "  #                                     batch_size = batch_size,\n",
    "  #                                     learning_rate = 1.0,\n",
    "  #                                     learning_rate_decay_factor = 0.9,\n",
    "  #                                     use_lstm = True,\n",
    "  #                                     forward_only = False)\n",
    "\n",
    "def create_model(session, forward_only):\n",
    "    \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "    model = seq2seq_model.Seq2SeqModel(\n",
    "        vocabulary_size, vocabulary_size, _buckets,\n",
    "        num_nodes, num_layers, max_gradient_norm, batch_size,\n",
    "        learning_rate, learning_rate_decay_factor,\n",
    "        use_lstm = True, forward_only=forward_only)\n",
    "    \n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should study this.\n",
    "\n",
    "def evaluate_model(model, sess, val_encoder_inputs, output_size):\n",
    "    val_decoder_inputs = np.zeros((output_size+1, batch_size,), dtype = np.int32)\n",
    "    val_target_weights = np.zeros((output_size+1, batch_size,), dtype = np.float32)\n",
    "    val_target_weights[0,:] = 1.0\n",
    "    is_finished = np.full((batch_size,), False, dtype = np.bool_)\n",
    "    for i in xrange(output_size):\n",
    "        _, _, output_logits = model.step(sess, val_encoder_inputs, val_decoder_inputs, val_target_weights,\n",
    "                                         bucket_id = 0, forward_only = True)\n",
    "        p = np.argmax(output_logits[i], axis = 1)\n",
    "        is_finished = np.logical_or(is_finished, p == 0)\n",
    "        val_decoder_inputs[i,:] = (1 - is_finished) * p\n",
    "        val_target_weights[i,:] = (1.0 - is_finished) * 1.0\n",
    "    return val_decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 501\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    # Create model.\n",
    "    print(\"Creating %d layers of %d units.\" % (num_layers, num_nodes))\n",
    "    model = create_model(sess, False)\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    for _ in xrange(num_steps):\n",
    "\n",
    "      # Get a batch and make a step.\n",
    "      encoder_inputs, decoder_inputs, target_weights = get_batch(10)\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                   target_weights, 0, False)\n",
    "      loss += step_loss / steps_per_checkpoint\n",
    "      current_step += 1\n",
    "\n",
    "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "      if current_step % steps_per_checkpoint == 0:\n",
    "        # Print statistics for the previous epoch.\n",
    "        perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "        print (\"step %d learning rate %.4f perplexity %.2f\" %\n",
    "               (model.global_step.eval(), model.learning_rate.eval(), perplexity))\n",
    "        # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "          sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "        # Zero timer and loss.\n",
    "        loss = 0.0\n",
    "        val_decoder_inputs = evaluate_model(model, sess, val_encoder_inputs, 10)\n",
    "        print(totext(val_decoder_inputs))\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1 layers of 128 units.\n",
      "Created model with fresh parameters.\n",
      "step 30 learning rate 0.5000 perplexity 17.91\n",
      "eeeeeeeeee kkkkkkkkkk nnnnnnnnnn xxxxxxxxxx ppppppmmmm ppeeeeeeee eeeeeeeeee xzzzzzzzzz gggggggggg jjj jjj jjj jjj jjj jjj jjj\n",
      "step 60 learning rate 0.5000 perplexity 11.49\n",
      "e c n o p r e y g       \n",
      "step 90 learning rate 0.5000 perplexity 8.05\n",
      "ee cc nnw ooo pppm rr ee y ggg       \n",
      "step 120 learning rate 0.5000 perplexity 5.73\n",
      "eee kkcccqq oowooo oooooo ssmmmjjj rrrvv eee yyy gggo       \n",
      "step 150 learning rate 0.5000 perplexity 4.67\n",
      "eeeehtt kkkkcii nnnnwww xxxxxx ssssppmu vvveev eeeehtt yyyyz gggoo       \n",
      "step 180 learning rate 0.5000 perplexity 3.86\n",
      "eht cciuq wwobb xo smmjj rvvo eht yzal ggd       \n",
      "step 210 learning rate 0.5000 perplexity 3.20\n",
      "eht kciuq nwobb xoo ppmmj revo eht yzal ggd       \n",
      "step 240 learning rate 0.5000 perplexity 2.47\n",
      "eht kciuq nwobb xof smmuj revo eht yzal god       \n",
      "step 270 learning rate 0.5000 perplexity 2.12\n",
      "eet kkkcu nnwob xxo sspmj rrev eet yyzl ggo       \n",
      "step 300 learning rate 0.5000 perplexity 2.01\n",
      "eht kciuq nwobb xx ssmuj revo eht yzal god       \n",
      "step 330 learning rate 0.5000 perplexity 1.64\n",
      "eht kkciu wwwrr xof sssuj revo eht yzal god       \n",
      "step 360 learning rate 0.5000 perplexity 1.77\n",
      "eht kkiuq nworb xof ssmuj revo eht yzal god       \n",
      "step 390 learning rate 0.5000 perplexity 1.43\n",
      "eht kciuq nworb xof spmuj revo eht yzal god       \n",
      "step 420 learning rate 0.5000 perplexity 1.71\n",
      "eht kciiq nwwrb xof sspmj rvvo eht yzal god       \n",
      "step 450 learning rate 0.5000 perplexity 1.49\n",
      "eht kciuq norrb xof spmuj revo eht yzal god       \n",
      "step 480 learning rate 0.5000 perplexity 1.25\n",
      "eht kciuq nworb xof spmuj revo eht yzal god       \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode():\n",
    "  with tf.Session() as sess:\n",
    "    # Create model and load parameters.\n",
    "    model = create_model(sess, True)\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "\n",
    "    # Load vocabularies.\n",
    "    en_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.en\" % FLAGS.en_vocab_size)\n",
    "    fr_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.fr\" % FLAGS.fr_vocab_size)\n",
    "    en_vocab, _ = data_utils.initialize_vocabulary(en_vocab_path)\n",
    "    _, rev_fr_vocab = data_utils.initialize_vocabulary(fr_vocab_path)\n",
    "\n",
    "    # Decode from standard input.\n",
    "    sys.stdout.write(\"> \")\n",
    "    sys.stdout.flush()\n",
    "    sentence = sys.stdin.readline()\n",
    "    while sentence:\n",
    "      # Get token-ids for the input sentence.\n",
    "      token_ids = data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence), en_vocab)\n",
    "      # Which bucket does it belong to?\n",
    "      bucket_id = min([b for b in xrange(len(_buckets))\n",
    "                       if _buckets[b][0] > len(token_ids)])\n",
    "      # Get a 1-element batch to feed the sentence to the model.\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          {bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "      # Get output logits for the sentence.\n",
    "      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "      # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "      # If there is an EOS symbol in outputs, cut them at that point.\n",
    "      if data_utils.EOS_ID in outputs:\n",
    "        outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
    "      # Print out French sentence corresponding to outputs.\n",
    "      print(\" \".join([tf.compat.as_str(rev_fr_vocab[output]) for output in outputs]))\n",
    "      print(\"> \", end=\"\")\n",
    "      sys.stdout.flush()\n",
    "      sentence = sys.stdin.readline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-test for neural translation model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "def self_test():\n",
    "    \"\"\"Test the translation model.\"\"\"\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(\"Self-test for neural translation model.\")\n",
    "        # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "        model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2,\n",
    "                                       5.0, 32, 0.3, 0.99, num_samples=8)\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # Fake data set for both the (3, 3) and (6, 6) bucket.\n",
    "        data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "                    [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "        for _ in xrange(5):  # Train the fake model for 5 steps.\n",
    "            bucket_id = random.choice([0, 1])\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(data_set, bucket_id)\n",
    "            model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "\n",
    "self_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
