{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse words\n",
    "=============\n",
    "\n",
    "Based on translate.py and on code from xxxxx.\n",
    "\n",
    "To do\n",
    "-----\n",
    "\n",
    "- clean old code.\n",
    "- remove save/restore model.\n",
    "- do predict.\n",
    "- check loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Binary for training translation models and decoding from them.\n",
    "\n",
    "See the following papers for more information on neural translation models.\n",
    " * http://arxiv.org/abs/1409.3215\n",
    " * http://arxiv.org/abs/1409.0473\n",
    " * http://arxiv.org/abs/1412.2007\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.translate import data_utils\n",
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "\n",
    "\n",
    "learning_rate = 0.5                  # \"Learning rate.\"\n",
    "learning_rate_decay_factor = 0.9     # \"Learning rate decays by this much.\"\n",
    "max_gradient_norm = 5.0              # \"Clip gradients to this norm.\"\n",
    "num_nodes = 128                      # \"Size of each model layer.\") # was 1024 \n",
    "num_layers = 1                       # \"Number of layers in the model.\") # was 3\n",
    "\n",
    "steps_per_checkpoint = 30            # \"How many training steps to do per checkpoint.\")\n",
    "dodecode = False                     #, \"Set to True for interactive decoding.\")\n",
    "\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See seq2seq_model.Seq2SeqModel for details of how they work.\n",
    "# _buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "_buckets = [(10, 11)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character:  \n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) # [a-z]\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(size):\n",
    "    encoder_inputs = [np.random.randint(1, vocabulary_size, size) for _ in xrange(batch_size)]\n",
    "    decoder_inputs = [np.zeros(size+1, dtype = np.int32) for _ in xrange(batch_size)]\n",
    "    weights = [np.ones(size+1, dtype = np.float32) for _ in xrange(batch_size)]\n",
    "    for i in xrange(batch_size):\n",
    "        r = random.randint(1, size-1)\n",
    "        encoder_inputs[i][r:] = 0\n",
    "        # Reverse the encoder input sequence, but leave a 0 at index 0 and at least one 0 at the end.\n",
    "        # These are the GO and EOS markers.\n",
    "        decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1]\n",
    "        weights[i][r+1:] = 0.0\n",
    "    return np.transpose(encoder_inputs), np.transpose(decoder_inputs), np.transpose(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bacthes are three matrices, encoder_inputs and decoder_inputs and weights. Weights are all zeroes.\n",
    "\n",
    "encoder_inputs have one entry per column (a fake word) with one letter per row.\n",
    "\n",
    "decoder_inputs (the \"labels\") are encoder_inputs reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_i\n",
      "[[15 22  6 10 19  7 14 13  4  3 20  6 13 18 20 11 22  8  1  1]\n",
      " [ 9 23 16 19 18 10  9 24 15 24 24 16 20  6 25 17  0 17 16  5]\n",
      " [10  4  8 23 24 19  7 16  7 24 10  4 18  9 20  4  0  6 21 13]\n",
      " [ 0  0  0 19 10  0  0  0 13 10  6 13  0 19 19 15  0  0 12 22]\n",
      " [ 0  0  0  2  0  0  0  0  9  9 13  0  0 18 18  3  0  0  0 18]\n",
      " [ 0  0  0 14  0  0  0  0  0  8  0  0  0 16  7 18  0  0  0 12]\n",
      " [ 0  0  0  7  0  0  0  0  0  8  0  0  0 14 18 17  0  0  0  6]\n",
      " [ 0  0  0  6  0  0  0  0  0 23  0  0  0  0  4  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "d_i\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [10  4  8  6 10 19  7 16  9 23 13 13 18 14  4 17 22  6 12  9]\n",
      " [ 9 23 16  7 24 10  9 24 13  8  6  4 20 16 18 18  0 17 21  6]\n",
      " [15 22  6 14 18  7 14 13  7  8 10 16 13 18  7  3  0  8 16 12]\n",
      " [ 0  0  0  2 19  0  0  0 15  9 24  6  0 19 18 15  0  0  1 18]\n",
      " [ 0  0  0 19  0  0  0  0  4 10 20  0  0  9 19  4  0  0  0 22]\n",
      " [ 0  0  0 23  0  0  0  0  0 24  0  0  0  6 20 17  0  0  0 13]\n",
      " [ 0  0  0 19  0  0  0  0  0 24  0  0  0 18 25 11  0  0  0  5]\n",
      " [ 0  0  0 10  0  0  0  0  0  3  0  0  0  0 20  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "batch_size=20\n",
    "b = get_batch(10)\n",
    "print('e_i')\n",
    "print(b[0])\n",
    "print('d_i')\n",
    "print(b[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 17  2  6 10 15 20 12  4  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 8 21 18 15 21 22  8  1 15  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  9 15 24 13  5  5 26  7  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  3 23  0 16 18  0 25  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 11 14  0 19  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_validation_batch(text, size):\n",
    "    words = text.split()[:batch_size]\n",
    "    encoder_inputs = [np.zeros(size, dtype = np.int32) for _ in xrange(batch_size)]\n",
    "    for i, word in enumerate(words):\n",
    "        l = min(len(word), size - 1)\n",
    "        for j in xrange(l):\n",
    "            encoder_inputs[i][j] = char2id(word[j])\n",
    "    return np.transpose(encoder_inputs)\n",
    "\n",
    "val_encoder_inputs = get_validation_batch(\"the quick brown fox jumps over the lazy dog\", 10)\n",
    "print(val_encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "Model\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  # model = seq2seq_model.Seq2SeqModel(source_vocab_size = vocabulary_size,\n",
    "  #                                    target_vocab_size = vocabulary_size,\n",
    "  #                                     buckets = [(10, 11)],\n",
    "  #                                     size = num_nodes,\n",
    "  #                                     num_layers = 1, # one encoding and one decoding LSTM\n",
    "  #                                     max_gradient_norm = 5.0,\n",
    "  #                                     batch_size = batch_size,\n",
    "  #                                     learning_rate = 1.0,\n",
    "  #                                     learning_rate_decay_factor = 0.9,\n",
    "  #                                     use_lstm = True,\n",
    "  #                                     forward_only = False)\n",
    "\n",
    "def create_model(session, forward_only):\n",
    "    \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "    model = seq2seq_model.Seq2SeqModel(\n",
    "        vocabulary_size, vocabulary_size, _buckets,\n",
    "        num_nodes, num_layers, max_gradient_norm, batch_size,\n",
    "        learning_rate, learning_rate_decay_factor,\n",
    "        use_lstm = True, forward_only=forward_only)\n",
    "    \n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    # Create model.\n",
    "    print(\"Creating %d layers of %d units.\" % (num_layers, num_nodes))\n",
    "    model = create_model(sess, False)\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    for _ in xrange(num_steps):\n",
    "\n",
    "      # Get a batch and make a step.\n",
    "      start_time = time.time()\n",
    "      encoder_inputs, decoder_inputs, target_weights = get_batch(10)\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                   target_weights, 0, False)\n",
    "      step_time += (time.time() - start_time) / steps_per_checkpoint\n",
    "      loss += step_loss / steps_per_checkpoint\n",
    "      current_step += 1\n",
    "\n",
    "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "      if current_step % steps_per_checkpoint == 0:\n",
    "        # Print statistics for the previous epoch.\n",
    "        perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "        print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                         step_time, perplexity))\n",
    "        # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "          sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "        # Zero timer and loss.\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        # Run evals on development set and print their perplexity.\n",
    "        for bucket_id in xrange(len(_buckets)):\n",
    "          if len(dev_set[bucket_id]) == 0:\n",
    "            print(\"  eval: empty bucket %d\" % (bucket_id))\n",
    "            continue\n",
    "          encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "              dev_set, bucket_id)\n",
    "          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "          eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\n",
    "          print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should study this.\n",
    "\n",
    "def evaluate_model(model, sess, val_encoder_inputs, output_size):\n",
    "    val_decoder_inputs = np.zeros((output_size+1, batch_size,), dtype = np.int32)\n",
    "    val_target_weights = np.zeros((output_size+1, batch_size,), dtype = np.float32)\n",
    "    val_target_weights[0,:] = 1.0\n",
    "    is_finished = np.full((batch_size,), False, dtype = np.bool_)\n",
    "    for i in xrange(output_size):\n",
    "        _, _, output_logits = model.step(sess, val_encoder_inputs, val_decoder_inputs, val_target_weights, bucket_id = 0, forward_only = True)\n",
    "        p = np.argmax(output_logits[i], axis = 1)\n",
    "        is_finished = np.logical_or(is_finished, p == 0)\n",
    "        val_decoder_inputs[i,:] = (1 - is_finished) * p\n",
    "        val_target_weights[i,:] = (1.0 - is_finished) * 1.0\n",
    "    return val_decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1 layers of 128 units.\n",
      "Created model with fresh parameters.\n",
      "global step 30 learning rate 0.5000 step-time 0.16 perplexity 17.05\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dev_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-af02a47b131e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-f0f0848f0c5d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Run evals on development set and print their perplexity.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbucket_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_buckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  eval: empty bucket %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_set' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode():\n",
    "  with tf.Session() as sess:\n",
    "    # Create model and load parameters.\n",
    "    model = create_model(sess, True)\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "\n",
    "    # Load vocabularies.\n",
    "    en_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.en\" % FLAGS.en_vocab_size)\n",
    "    fr_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.fr\" % FLAGS.fr_vocab_size)\n",
    "    en_vocab, _ = data_utils.initialize_vocabulary(en_vocab_path)\n",
    "    _, rev_fr_vocab = data_utils.initialize_vocabulary(fr_vocab_path)\n",
    "\n",
    "    # Decode from standard input.\n",
    "    sys.stdout.write(\"> \")\n",
    "    sys.stdout.flush()\n",
    "    sentence = sys.stdin.readline()\n",
    "    while sentence:\n",
    "      # Get token-ids for the input sentence.\n",
    "      token_ids = data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence), en_vocab)\n",
    "      # Which bucket does it belong to?\n",
    "      bucket_id = min([b for b in xrange(len(_buckets))\n",
    "                       if _buckets[b][0] > len(token_ids)])\n",
    "      # Get a 1-element batch to feed the sentence to the model.\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          {bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "      # Get output logits for the sentence.\n",
    "      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "      # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "      # If there is an EOS symbol in outputs, cut them at that point.\n",
    "      if data_utils.EOS_ID in outputs:\n",
    "        outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
    "      # Print out French sentence corresponding to outputs.\n",
    "      print(\" \".join([tf.compat.as_str(rev_fr_vocab[output]) for output in outputs]))\n",
    "      print(\"> \", end=\"\")\n",
    "      sys.stdout.flush()\n",
    "      sentence = sys.stdin.readline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-test for neural translation model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "def self_test():\n",
    "    \"\"\"Test the translation model.\"\"\"\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(\"Self-test for neural translation model.\")\n",
    "        # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "        model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2,\n",
    "                                       5.0, 32, 0.3, 0.99, num_samples=8)\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # Fake data set for both the (3, 3) and (6, 6) bucket.\n",
    "        data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "                    [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "        for _ in xrange(5):  # Train the fake model for 5 steps.\n",
    "            bucket_id = random.choice([0, 1])\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(data_set, bucket_id)\n",
    "            model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "\n",
    "self_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
