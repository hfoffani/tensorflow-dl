{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse words\n",
    "=============\n",
    "\n",
    "Based on translate.py and on code from xxxxx.\n",
    "\n",
    "To do\n",
    "-----\n",
    "\n",
    "- clean old code.\n",
    "- do predict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Binary for training translation models and decoding from them.\n",
    "\n",
    "See the following papers for more information on neural translation models.\n",
    " * http://arxiv.org/abs/1409.3215\n",
    " * http://arxiv.org/abs/1409.0473\n",
    " * http://arxiv.org/abs/1412.2007\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.translate import data_utils\n",
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "\n",
    "\n",
    "learning_rate = 0.5                  # \"Learning rate.\"\n",
    "learning_rate_decay_factor = 0.9     # \"Learning rate decays by this much.\"\n",
    "max_gradient_norm = 5.0              # \"Clip gradients to this norm.\"\n",
    "num_nodes = 128                      # \"Size of each model layer.\" # was 1024 \n",
    "num_layers = 1                       # \"Number of layers in the model.\" # was 3\n",
    "\n",
    "steps_per_eval = 10                  # \"How many training steps to do per eval.\"\n",
    "evals_per_validate = 10\n",
    "dodecode = False                     # \"Set to True for interactive decoding.\"\n",
    "\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See seq2seq_model.Seq2SeqModel for details of how they work.\n",
    "# _buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "_buckets = [(10, 11)]\n",
    "\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character:  \n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z]\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(size):\n",
    "    encoder_inputs = [np.random.randint(1, vocabulary_size, size) for _ in xrange(batch_size)]\n",
    "    decoder_inputs = [np.zeros(size+1, dtype = np.int32) for _ in xrange(batch_size)]\n",
    "    weights = [np.ones(size+1, dtype = np.float32) for _ in xrange(batch_size)]\n",
    "    for i in xrange(batch_size):\n",
    "        r = random.randint(1, size-1)\n",
    "        encoder_inputs[i][r:] = 0\n",
    "        # Reverse the encoder input sequence, but leave a 0 at index 0 and at least one 0 at the end.\n",
    "        # These are the GO and EOS markers.\n",
    "        decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1]\n",
    "        weights[i][r+1:] = 0.0\n",
    "    return np.transpose(encoder_inputs), np.transpose(decoder_inputs), np.transpose(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bacthes are three matrices, encoder_inputs and decoder_inputs and weights. Weights are all zeroes.\n",
    "\n",
    "encoder_inputs have one entry per column (a fake word) with one letter per row.\n",
    "\n",
    "decoder_inputs (the \"labels\") are encoder_inputs reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_i\n",
      "[[21 18 18  3 10  2 22 18 14  5  5 23  6  4  7 16]\n",
      " [10  0 22 24 12  9 10 23 19  9  1 18 16  4 12  0]\n",
      " [25  0 12 10 17 25  0  1  0  4 20  7 16  6 24  0]\n",
      " [12  0 23  5  0 10  0 10  0  0 16 15  4  0 24  0]\n",
      " [25  0 13 16  0  9  0 23  0  0 19  5  6  0 24  0]\n",
      " [13  0 22 19  0 12  0  4  0  0  0 13  0  0  1  0]\n",
      " [ 0  0 20 25  0  0  0 10  0  0  0  0  0  0  8  0]\n",
      " [ 0  0  9  3  0  0  0  0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0 23 14  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "d_i\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13 18 23 14 17 12 10 10 19  4 19 13  6  6  1 16]\n",
      " [25  0  9  3 12  9 22  4 14  9 16  5  4  4  8  0]\n",
      " [12  0 20 25 10 10  0 23  0  5 20 15 16  4  1  0]\n",
      " [25  0 22 19  0 25  0 10  0  0  1  7 16  0 24  0]\n",
      " [10  0 13 16  0  9  0  1  0  0  5 18  6  0 24  0]\n",
      " [21  0 23  5  0  2  0 23  0  0  0 23  0  0 24  0]\n",
      " [ 0  0 12 10  0  0  0 18  0  0  0  0  0  0 12  0]\n",
      " [ 0  0 22 24  0  0  0  0  0  0  0  0  0  0  7  0]\n",
      " [ 0  0 18  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# batch_size=20\n",
    "b = get_batch(10)\n",
    "print('e_i')\n",
    "print(b[0])\n",
    "print('d_i')\n",
    "print(b[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 17  2  6 10 15 20 12  4  0  0  0  0  0  0  0]\n",
      " [ 8 21 18 15 21 22  8  1 15  0  0  0  0  0  0  0]\n",
      " [ 5  9 15 24 13  5  5 26  7  0  0  0  0  0  0  0]\n",
      " [ 0  3 23  0 16 18  0 25  0  0  0  0  0  0  0  0]\n",
      " [ 0 11 14  0 19  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "the quick brown fox jumps over the lazy dog       \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_validation_batch(text, size):\n",
    "    words = text.split()[:batch_size]\n",
    "    encoder_inputs = [np.zeros(size, dtype = np.int32) for _ in xrange(batch_size)]\n",
    "    for i, word in enumerate(words):\n",
    "        l = min(len(word), size - 1)\n",
    "        for j in xrange(l):\n",
    "            encoder_inputs[i][j] = char2id(word[j])\n",
    "    return np.transpose(encoder_inputs)\n",
    "\n",
    "def totext(inputs):\n",
    "    inp = np.transpose(inputs)\n",
    "    toletters = np.vectorize(id2char)\n",
    "    return ' '.join([ ''.join(row).rstrip() for row in toletters(inp)])\n",
    "\n",
    "\n",
    "val_encoder_inputs = get_validation_batch(\"the quick brown fox jumps over the lazy dog\", 10)\n",
    "print(val_encoder_inputs)\n",
    "print(totext(val_encoder_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "Model\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model(session, forward_only):\n",
    "    \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "    model = seq2seq_model.Seq2SeqModel(\n",
    "        vocabulary_size, vocabulary_size, _buckets,\n",
    "        num_nodes, num_layers, max_gradient_norm, batch_size,\n",
    "        learning_rate, learning_rate_decay_factor,\n",
    "        use_lstm = True, forward_only=forward_only)\n",
    "    \n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should study this.\n",
    "\n",
    "def evaluate_model(model, sess, val_encoder_inputs, output_size):\n",
    "    val_decoder_inputs = np.zeros((output_size+1, batch_size,), dtype = np.int32)\n",
    "    val_target_weights = np.zeros((output_size+1, batch_size,), dtype = np.float32)\n",
    "    val_target_weights[0,:] = 1.0\n",
    "    is_finished = np.full((batch_size,), False, dtype = np.bool_)\n",
    "    for i in xrange(output_size):\n",
    "        _, _, output_logits = model.step(sess,\n",
    "                                         val_encoder_inputs, val_decoder_inputs, val_target_weights,\n",
    "                                         bucket_id = 0, forward_only = True)\n",
    "        p = np.argmax(output_logits[i], axis = 1)\n",
    "        is_finished = np.logical_or(is_finished, p == 0)\n",
    "        val_decoder_inputs[i,:] = (1 - is_finished) * p\n",
    "        val_target_weights[i,:] = (1.0 - is_finished) * 1.0\n",
    "    return val_decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 501\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Create model.\n",
    "        print(\"Creating %d layers of %d units.\" % (num_layers, num_nodes))\n",
    "        model = create_model(sess, False)\n",
    "\n",
    "        # This is the training loop.\n",
    "        loss = 0.0\n",
    "        current_step = 0\n",
    "        previous_losses = []\n",
    "        for _ in xrange(num_steps):\n",
    "\n",
    "            # Get a batch and make a step.\n",
    "            encoder_inputs, decoder_inputs, target_weights = get_batch(10)\n",
    "            _, step_loss, _ = model.step(sess,\n",
    "                                         encoder_inputs, decoder_inputs, target_weights,\n",
    "                                         0, False)\n",
    "            loss += step_loss / steps_per_eval\n",
    "            current_step += 1\n",
    "\n",
    "            # Once in a while, print statistics, and run evals.\n",
    "            if current_step % steps_per_eval == 0:\n",
    "                # Print statistics for the previous epoch.\n",
    "                perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "                print (\"step %d learning rate %.4f perplexity %.2f\" %\n",
    "                        (model.global_step.eval(), model.learning_rate.eval(), perplexity))\n",
    "                \n",
    "                # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                    sess.run(model.learning_rate_decay_op)\n",
    "                previous_losses.append(loss)\n",
    "                # Zero loss.\n",
    "                loss = 0.0\n",
    "                \n",
    "                # test model.\n",
    "                if current_step % (steps_per_eval * evals_per_validate) == 0:\n",
    "                    val_decoder_inputs = evaluate_model(model, sess, val_encoder_inputs, 10)\n",
    "                    print(totext(val_decoder_inputs))\n",
    "                \n",
    "                sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1 layers of 128 units.\n",
      "Created model with fresh parameters.\n",
      "step 10 learning rate 0.5000 perplexity 22.90\n",
      "step 20 learning rate 0.5000 perplexity 18.16\n",
      "step 30 learning rate 0.5000 perplexity 15.30\n",
      "step 40 learning rate 0.5000 perplexity 15.38\n",
      "step 50 learning rate 0.5000 perplexity 11.10\n",
      "step 60 learning rate 0.5000 perplexity 10.80\n",
      "step 70 learning rate 0.5000 perplexity 9.55\n",
      "step 80 learning rate 0.5000 perplexity 7.92\n",
      "step 90 learning rate 0.5000 perplexity 6.07\n",
      "step 100 learning rate 0.5000 perplexity 6.55\n",
      "ee kci nnbb xf pp re ee yy gg       \n",
      "step 110 learning rate 0.5000 perplexity 5.75\n",
      "step 120 learning rate 0.5000 perplexity 5.16\n",
      "step 130 learning rate 0.5000 perplexity 5.16\n",
      "step 140 learning rate 0.5000 perplexity 5.12\n",
      "step 150 learning rate 0.5000 perplexity 4.31\n",
      "step 160 learning rate 0.5000 perplexity 3.63\n",
      "step 170 learning rate 0.5000 perplexity 4.24\n",
      "step 180 learning rate 0.5000 perplexity 3.68\n",
      "step 190 learning rate 0.5000 perplexity 3.58\n",
      "step 200 learning rate 0.5000 perplexity 3.08\n",
      "ett kcuuq nwrbb xf spujj revo ett yaa gd       \n",
      "step 210 learning rate 0.5000 perplexity 3.08\n",
      "step 220 learning rate 0.5000 perplexity 3.22\n",
      "step 230 learning rate 0.5000 perplexity 2.79\n",
      "step 240 learning rate 0.5000 perplexity 2.09\n",
      "step 250 learning rate 0.5000 perplexity 3.07\n",
      "step 260 learning rate 0.5000 perplexity 2.64\n",
      "step 270 learning rate 0.5000 perplexity 2.34\n",
      "step 280 learning rate 0.5000 perplexity 2.35\n",
      "step 290 learning rate 0.5000 perplexity 1.86\n",
      "step 300 learning rate 0.5000 perplexity 1.91\n",
      "eht kciuq nworb xof spmuj revo eht yaal god       \n",
      "step 310 learning rate 0.5000 perplexity 1.88\n",
      "step 320 learning rate 0.5000 perplexity 2.08\n",
      "step 330 learning rate 0.4500 perplexity 1.78\n",
      "step 340 learning rate 0.4500 perplexity 1.77\n",
      "step 350 learning rate 0.4500 perplexity 1.67\n",
      "step 360 learning rate 0.4500 perplexity 1.44\n",
      "step 370 learning rate 0.4500 perplexity 1.33\n",
      "step 380 learning rate 0.4500 perplexity 1.39\n",
      "step 390 learning rate 0.4500 perplexity 2.30\n",
      "step 400 learning rate 0.4050 perplexity 1.57\n",
      "eht kciuq nworb xof spmuj reoo eht yzal god       \n",
      "step 410 learning rate 0.4050 perplexity 1.34\n",
      "step 420 learning rate 0.4050 perplexity 1.24\n",
      "step 430 learning rate 0.4050 perplexity 1.56\n",
      "step 440 learning rate 0.4050 perplexity 1.49\n",
      "step 450 learning rate 0.4050 perplexity 1.25\n",
      "step 460 learning rate 0.4050 perplexity 1.96\n",
      "step 470 learning rate 0.3645 perplexity 1.23\n",
      "step 480 learning rate 0.3645 perplexity 1.16\n",
      "step 490 learning rate 0.3645 perplexity 1.27\n",
      "step 500 learning rate 0.3645 perplexity 1.17\n",
      "eht kiiuq nworb xof spmuj revo eht yaal god       \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode():\n",
    "  with tf.Session() as sess:\n",
    "    # Create model and load parameters.\n",
    "    model = create_model(sess, True)\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "\n",
    "    # Load vocabularies.\n",
    "    en_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.en\" % FLAGS.en_vocab_size)\n",
    "    fr_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.fr\" % FLAGS.fr_vocab_size)\n",
    "    en_vocab, _ = data_utils.initialize_vocabulary(en_vocab_path)\n",
    "    _, rev_fr_vocab = data_utils.initialize_vocabulary(fr_vocab_path)\n",
    "\n",
    "    # Decode from standard input.\n",
    "    sys.stdout.write(\"> \")\n",
    "    sys.stdout.flush()\n",
    "    sentence = sys.stdin.readline()\n",
    "    while sentence:\n",
    "      # Get token-ids for the input sentence.\n",
    "      token_ids = data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence), en_vocab)\n",
    "      # Which bucket does it belong to?\n",
    "      bucket_id = min([b for b in xrange(len(_buckets))\n",
    "                       if _buckets[b][0] > len(token_ids)])\n",
    "      # Get a 1-element batch to feed the sentence to the model.\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          {bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "      # Get output logits for the sentence.\n",
    "      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "      # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "      # If there is an EOS symbol in outputs, cut them at that point.\n",
    "      if data_utils.EOS_ID in outputs:\n",
    "        outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
    "      # Print out French sentence corresponding to outputs.\n",
    "      print(\" \".join([tf.compat.as_str(rev_fr_vocab[output]) for output in outputs]))\n",
    "      print(\"> \", end=\"\")\n",
    "      sys.stdout.flush()\n",
    "      sentence = sys.stdin.readline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-test for neural translation model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "def self_test():\n",
    "    \"\"\"Test the translation model.\"\"\"\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(\"Self-test for neural translation model.\")\n",
    "        # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "        model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2,\n",
    "                                       5.0, 32, 0.3, 0.99, num_samples=8)\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # Fake data set for both the (3, 3) and (6, 6) bucket.\n",
    "        data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "                    [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "        for _ in xrange(5):  # Train the fake model for 5 steps.\n",
    "            bucket_id = random.choice([0, 1])\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(data_set, bucket_id)\n",
    "            model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "\n",
    "self_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
