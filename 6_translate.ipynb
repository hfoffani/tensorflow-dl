{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse words\n",
    "=============\n",
    "\n",
    "Based on translate.py and on code from xxxxx.\n",
    "\n",
    "To do\n",
    "-----\n",
    "\n",
    "- show chart losses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Binary for training translation models and decoding from them.\n",
    "\n",
    "See the following papers for more information on neural translation models.\n",
    " * http://arxiv.org/abs/1409.3215\n",
    " * http://arxiv.org/abs/1409.0473\n",
    " * http://arxiv.org/abs/1412.2007\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn.translate import data_utils\n",
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "\n",
    "\n",
    "learning_rate = 0.5                  # \"Learning rate.\"\n",
    "learning_rate_decay_factor = 0.9     # \"Learning rate decays by this much.\"\n",
    "max_gradient_norm = 5.0              # \"Clip gradients to this norm.\"\n",
    "num_nodes = 128                      # \"Size of each model layer.\" # was 1024 \n",
    "num_layers = 1                       # \"Number of layers in the model.\" # was 3\n",
    "\n",
    "steps_per_eval = 10                  # \"How many training steps to do per eval.\"\n",
    "evals_per_validate = 10\n",
    "dodecode = False                     # \"Set to True for interactive decoding.\"\n",
    "\n",
    "\n",
    "batch_size = 16                      # equals to max number of words per input.\n",
    "MAX_CHARS_PER_WORD = 10\n",
    "\n",
    "# We use a number of buckets and pad to the closest one for efficiency.\n",
    "# See seq2seq_model.Seq2SeqModel for details of how they work.\n",
    "# _buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
    "_buckets = [(MAX_CHARS_PER_WORD, MAX_CHARS_PER_WORD+1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character:  \n",
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z]\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(size):\n",
    "    encoder_inputs = [np.random.randint(1, vocabulary_size, size) for _ in xrange(batch_size)]\n",
    "    decoder_inputs = [np.zeros(size+1, dtype = np.int32) for _ in xrange(batch_size)]\n",
    "    weights = [np.ones(size+1, dtype = np.float32) for _ in xrange(batch_size)]\n",
    "    for i in xrange(batch_size):\n",
    "        r = random.randint(1, size-1)\n",
    "        encoder_inputs[i][r:] = 0\n",
    "        # Reverse the encoder input sequence, but leave a 0 at index 0 and at least one 0 at the end.\n",
    "        # These are the GO and EOS markers.\n",
    "        decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1]\n",
    "        weights[i][r+1:] = 0.0\n",
    "    return np.transpose(encoder_inputs), np.transpose(decoder_inputs), np.transpose(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bacthes are three matrices, encoder_inputs and decoder_inputs and weights. Weights are all zeroes.\n",
    "\n",
    "encoder_inputs have one entry per column (a fake word) with one letter per row.\n",
    "\n",
    "decoder_inputs (the \"labels\") are encoder_inputs reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_i\n",
      "[[11 15 24  7 19 16 25  4  2  2  5 16 20 25 26 25]\n",
      " [ 8  3  2 18  0 19 14  0 26  0 19  7 21 11  9  7]\n",
      " [22 11 18  8  0 20 23  0  0  0 14 21 14  8  3 14]\n",
      " [18 20  4  7  0  8 16  0  0  0 17 16  1 11  7  0]\n",
      " [ 4  5 25 15  0  9  6  0  0  0  2  1  4 22 15  0]\n",
      " [ 0  9 13 20  0 25 24  0  0  0 24 21  0  0 16  0]\n",
      " [ 0  4  0  6  0  0  0  0  0  0  6 21  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 17  7  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 20 18  0  0 16  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "d_i\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  4 13  6 19 25 24  4 26  2 20 18  4 22 16 14]\n",
      " [18  9 25 20  0  9  6  0  2  0 17  7  1 11  1  7]\n",
      " [22  5  4 15  0  8 16  0  0  0  6 21 14  8 19 25]\n",
      " [ 8 20 18  7  0 20 23  0  0  0 24 21 21 11 16  0]\n",
      " [11 11  2  8  0 19 14  0  0  0  2  1 20 25 15  0]\n",
      " [ 0  3 24 18  0 16 25  0  0  0 17 16  0  0  7  0]\n",
      " [ 0 15  0  7  0  0  0  0  0  0 14 21  0  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 19  7  0  0  9  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  5 16  0  0 26  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# batch_size=20\n",
    "b = get_batch(MAX_CHARS_PER_WORD)\n",
    "print('e_i')\n",
    "print(b[0])\n",
    "print('d_i')\n",
    "print(b[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 17  2  6 10 15 20 12  4  2 25  1 13  2 14  8]\n",
      " [ 8 21 18 15 21 22  8  1 15  5 15 14  5 21 15  5]\n",
      " [ 5  9 15 24 13  5  5 26  7 20 21  4  0 20 20 18]\n",
      " [ 0  3 23  0 16 18  0 25  0 23  0  0  0  0  0  0]\n",
      " [ 0 11 14  0 19  0  0  0  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 14  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "> [the quick brown fox jumps over the lazy dog between you and me but not her]\n",
      "< [the quick brown fox jumps over the lazy dog between you and me but not her]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_validation_batch(text, size):\n",
    "    words = text.split()[:batch_size]\n",
    "    encoder_inputs = [np.zeros(size, dtype = np.int32) for _ in xrange(batch_size)]\n",
    "    for i, word in enumerate(words):\n",
    "        l = min(len(word), size - 1)\n",
    "        for j in xrange(l):\n",
    "            encoder_inputs[i][j] = char2id(word[j])\n",
    "    return np.transpose(encoder_inputs)\n",
    "\n",
    "def totext(inputs):\n",
    "    inp = np.transpose(inputs)\n",
    "    toletters = np.vectorize(id2char)\n",
    "    return ' '.join([ ''.join(row).rstrip() for row in toletters(inp)]).rstrip()\n",
    "\n",
    "\n",
    "text_encoded = \"the quick brown fox jumps over the lazy dog between you and me but not her\"\n",
    "val_encoder_inputs = get_validation_batch(text_encoded, MAX_CHARS_PER_WORD)\n",
    "print(val_encoder_inputs)\n",
    "print(\"> [%s]\" % text_encoded)\n",
    "print(\"< [%s]\" % totext(val_encoder_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "Model\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_model(session, forward_only):\n",
    "    \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "    model = seq2seq_model.Seq2SeqModel(\n",
    "        vocabulary_size, vocabulary_size, _buckets,\n",
    "        num_nodes, num_layers, max_gradient_norm, batch_size,\n",
    "        learning_rate, learning_rate_decay_factor,\n",
    "        use_lstm = True, forward_only=forward_only)\n",
    "    \n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# should study this.\n",
    "\n",
    "def evaluate_model(model, sess, val_encoder_inputs, output_size):\n",
    "    val_decoder_inputs = np.zeros((output_size+1, batch_size,), dtype = np.int32)\n",
    "    val_target_weights = np.zeros((output_size+1, batch_size,), dtype = np.float32)\n",
    "    val_target_weights[0,:] = 1.0\n",
    "    is_finished = np.full((batch_size,), False, dtype = np.bool_)\n",
    "    for i in xrange(output_size):\n",
    "        _, _, output_logits = model.step(sess,\n",
    "                                         val_encoder_inputs, val_decoder_inputs, val_target_weights,\n",
    "                                         bucket_id = 0, forward_only = True)\n",
    "        p = np.argmax(output_logits[i], axis = 1)\n",
    "        is_finished = np.logical_or(is_finished, p == 0)\n",
    "        val_decoder_inputs[i,:] = (1 - is_finished) * p\n",
    "        val_target_weights[i,:] = (1.0 - is_finished) * 1.0\n",
    "    return val_decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 501\n",
    "\n",
    "\n",
    "def train():\n",
    "    step_losses = []\n",
    "    with tf.Session() as sess:\n",
    "        # Create model.\n",
    "        print(\"Creating %d layers of %d units.\" % (num_layers, num_nodes))\n",
    "        model = create_model(sess, False)\n",
    "\n",
    "        # This is the training loop.\n",
    "        loss = 0.0\n",
    "        current_step = 0\n",
    "        previous_losses = []\n",
    "        for _ in xrange(num_steps):\n",
    "\n",
    "            # Get a batch and make a step.\n",
    "            encoder_inputs, decoder_inputs, target_weights = get_batch(MAX_CHARS_PER_WORD)\n",
    "            _, step_loss, _ = model.step(sess,\n",
    "                                         encoder_inputs, decoder_inputs, target_weights,\n",
    "                                         0, False)\n",
    "            step_losses.append(step_loss)\n",
    "            loss += step_loss / steps_per_eval\n",
    "            current_step += 1\n",
    "\n",
    "            # Once in a while, print statistics, and run evals.\n",
    "            if current_step % steps_per_eval == 0:\n",
    "                # Print statistics for the previous epoch.\n",
    "                perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "                print (\"step %d learning rate %.4f perplexity %.2f\" %\n",
    "                        (model.global_step.eval(), model.learning_rate.eval(), perplexity))\n",
    "                \n",
    "                # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                    sess.run(model.learning_rate_decay_op)\n",
    "                previous_losses.append(loss)\n",
    "                # Zero loss.\n",
    "                loss = 0.0\n",
    "                \n",
    "                # test model.\n",
    "                if current_step % (steps_per_eval * evals_per_validate) == 0:\n",
    "                    val_decoder_inputs = evaluate_model(model, sess, val_encoder_inputs, MAX_CHARS_PER_WORD)\n",
    "                    text_decoded = totext(val_decoder_inputs)\n",
    "                    print(\"> [%s]\" % text_encoded)\n",
    "                    print(\"< [%s]\" % text_decoded)\n",
    "                \n",
    "                sys.stdout.flush()\n",
    "    return step_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1 layers of 128 units.\n",
      "Created model with fresh parameters.\n",
      "step 10 learning rate 0.5000 perplexity 21.68\n",
      "step 20 learning rate 0.5000 perplexity 17.96\n",
      "step 30 learning rate 0.5000 perplexity 14.35\n",
      "step 40 learning rate 0.5000 perplexity 15.00\n",
      "step 50 learning rate 0.5000 perplexity 11.33\n",
      "step 60 learning rate 0.5000 perplexity 9.38\n",
      "step 70 learning rate 0.5000 perplexity 8.93\n",
      "step 80 learning rate 0.5000 perplexity 7.26\n",
      "step 90 learning rate 0.5000 perplexity 7.76\n",
      "step 100 learning rate 0.5000 perplexity 5.83\n",
      "> [the quick brown fox jumps over the lazy dog between you and me but not her]\n",
      "< [e iiii ww x mmmjj rr e za g eeewt oy da e t o r]\n",
      "step 110 learning rate 0.5000 perplexity 5.48\n",
      "step 120 learning rate 0.5000 perplexity 5.25\n",
      "step 130 learning rate 0.5000 perplexity 4.88\n",
      "step 140 learning rate 0.5000 perplexity 4.83\n",
      "step 150 learning rate 0.5000 perplexity 4.22\n",
      "step 160 learning rate 0.5000 perplexity 4.21\n",
      "step 170 learning rate 0.5000 perplexity 3.98\n",
      "step 180 learning rate 0.5000 perplexity 3.66\n",
      "step 190 learning rate 0.5000 perplexity 3.34\n",
      "step 200 learning rate 0.5000 perplexity 3.23\n",
      "> [the quick brown fox jumps over the lazy dog between you and me but not her]\n",
      "< [ehh kciiu nnwrb xxf pppmuu rrvo ehh yzall god eeeeeee uuy dnaa em tuu ttn rrh]\n",
      "step 210 learning rate 0.5000 perplexity 3.24\n",
      "step 220 learning rate 0.5000 perplexity 2.94\n",
      "step 230 learning rate 0.5000 perplexity 2.81\n",
      "step 240 learning rate 0.5000 perplexity 2.62\n",
      "step 250 learning rate 0.5000 perplexity 2.16\n",
      "step 260 learning rate 0.5000 perplexity 2.71\n",
      "step 270 learning rate 0.5000 perplexity 2.31\n",
      "step 280 learning rate 0.5000 perplexity 1.55\n",
      "step 290 learning rate 0.5000 perplexity 2.04\n",
      "step 300 learning rate 0.5000 perplexity 1.80\n",
      "> [the quick brown fox jumps over the lazy dog between you and me but not her]\n",
      "< [eht kiiuq nwrbb xxf spmuj revo eht yzal god eeetebb uoy dna em tub ton reh]\n",
      "step 310 learning rate 0.5000 perplexity 1.63\n",
      "step 320 learning rate 0.5000 perplexity 1.84\n",
      "step 330 learning rate 0.5000 perplexity 1.57\n",
      "step 340 learning rate 0.5000 perplexity 1.55\n",
      "step 350 learning rate 0.5000 perplexity 1.64\n",
      "step 360 learning rate 0.5000 perplexity 1.91\n",
      "step 370 learning rate 0.4500 perplexity 1.37\n",
      "step 380 learning rate 0.4500 perplexity 1.34\n",
      "step 390 learning rate 0.4500 perplexity 1.51\n",
      "step 400 learning rate 0.4500 perplexity 1.39\n",
      "> [the quick brown fox jumps over the lazy dog between you and me but not her]\n",
      "< [eht kciuq nworb xof spmuj revo eht yzal god eeewteb uoy dna em tub ton reh]\n",
      "step 410 learning rate 0.4500 perplexity 1.22\n",
      "step 420 learning rate 0.4500 perplexity 2.26\n",
      "step 430 learning rate 0.4050 perplexity 1.47\n",
      "step 440 learning rate 0.4050 perplexity 1.23\n",
      "step 450 learning rate 0.4050 perplexity 1.29\n",
      "step 460 learning rate 0.4050 perplexity 1.17\n",
      "step 470 learning rate 0.4050 perplexity 1.10\n",
      "step 480 learning rate 0.4050 perplexity 1.19\n",
      "step 490 learning rate 0.4050 perplexity 1.30\n",
      "step 500 learning rate 0.3645 perplexity 1.13\n",
      "> [the quick brown fox jumps over the lazy dog between you and me but not her]\n",
      "< [eht kciuq nworb xof spmuj revo eht yzll god neewteb uoy dna em tub ton reh]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "all_losses = train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-test for neural translation model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "def self_test():\n",
    "    \"\"\"Test the translation model.\"\"\"\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        print(\"Self-test for neural translation model.\")\n",
    "        # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "        model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2,\n",
    "                                       5.0, 32, 0.3, 0.99, num_samples=8)\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # Fake data set for both the (3, 3) and (6, 6) bucket.\n",
    "        data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "                    [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "        for _ in xrange(5):  # Train the fake model for 5 steps.\n",
    "            bucket_id = random.choice([0, 1])\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(data_set, bucket_id)\n",
    "            model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "\n",
    "self_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
