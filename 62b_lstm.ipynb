{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train is list of unrollings+1, each a np.array(64,27) of a batch of one-hot encodings\n",
      "train batch: 12 array of (64, 27)\n",
      "to-id is  list of unrollings+1, each a np.array(64) of a batch of id-embeddings\n",
      "to_id batch: 12 array of (64,)\n",
      "to bigrams [419 629 336   1 352 221 675  52 549 360 383 221 135 137 405 680 423  27\n",
      "  20  45 263   1 549 133 162  47 135 149  40 508 258 405  27 197 257 501\n",
      "  82  46   4 366 540 567 135 405 411 513 309 153 640 155 548 155  19 262\n",
      " 549 108 548 149 167 129 558  47  43 522]\n",
      "[array([19, 18,  5,  0,  1,  8,  7, 15,  8, 15,  8,  5, 20, 15, 15,  5,  3,\n",
      "        9,  7,  0, 12,  5,  0,  8, 18, 20, 20, 13,  0, 20, 19,  5,  0,  0,\n",
      "        0,  8, 19,  5, 12, 15,  0,  4, 14, 20, 14,  0, 19, 21, 20,  9, 19,\n",
      "       12, 20, 18,  9,  0, 19,  3,  9, 13,  3,  0,  5,  5]), array([20, 25, 19, 13,  3,  1,  9, 18,  5, 15,  5, 22,  8,  2,  0,  9,  1,\n",
      "       20,  8, 21, 15, 12, 19,  9, 21,  1,  8,  0,  2,  1, 21,  0, 15,  8,\n",
      "        5,  5,  0,  0,  0, 18, 15,  9,  7, 25,  7,  3, 19, 24,  8,  0, 15,\n",
      "        1, 23,  0, 20, 13, 11,  0, 18,  0,  5, 20, 22,  0]), array([19,  0,  0, 15,  1, 18,  3,  0,  0, 11, 18,  5,  0,  1, 18, 22, 14,\n",
      "        9, 20, 14, 19, 12,  9, 13,  7, 11,  5, 20,  1, 14,  3, 15,  6,  9,\n",
      "        9,  0,  3, 20,  1, 13, 18, 19,  0, 16, 21, 15,  0,  0,  5,  3,  3,\n",
      "       20, 15,  8,  9, 15,  5, 15,  0,  1, 14,  8, 15, 19]), array([ 0,  7, 14, 14,  0,  4,  1, 16, 14,  0,  0, 14,  1,  2,  5,  5, 20,\n",
      "        3,  0,  3, 20, 21, 26,  0, 19,  5,  0, 15, 18,  4,  8, 14,  0, 22,\n",
      "        7, 12, 12,  8, 14, 15,  0,  1, 19,  5,  1, 13, 15, 19,  0, 15,  9,\n",
      "        9, 18,  9,  3, 19, 18, 22,  3,  3, 20,  1, 20, 21]), array([ 1, 15,  1,  1, 16,  0, 12,  1,  1, 16, 23,  0,  0, 12,  3,  4,  0,\n",
      "        0,  9,  1,  0, 12,  5,  1,  0,  0, 16,  0, 18,  1,  0,  0, 20,  5,\n",
      "        8,  5,  1,  5,  1, 14,  1,  7, 25, 19,  7, 13, 14, 21,  6, 14,  5,\n",
      "       22, 11, 18,  1, 20,  4,  5, 15, 14,  5, 14,  9,  3]), array([ 4, 22, 20, 19, 18,  2,  0, 19, 20, 12,  5, 19,  7, 25, 15,  0, 20,\n",
      "       15, 14, 21,  1,  1,  0,  0,  3, 20, 18, 14,  5, 18,  1, 20,  8, 18,\n",
      "       20,  1, 19,  0, 12, 19, 20, 18, 19,  0,  5,  9,  5, 19,  9,  3, 20,\n",
      "        5, 19, 15, 12,  0, 15, 18, 13, 13, 18,  0, 15,  8]), array([22,  5,  9, 20,  9,  1, 12, 19,  9,  1, 12,  9, 12,  0,  7, 20,  8,\n",
      "        6,  0, 19, 19, 18, 15, 19, 15, 15,  9,  1,  4,  4, 19,  8,  5,  0,\n",
      "        0,  4, 19, 14, 25,  0,  0,  5, 20,  2, 19, 19,  0,  5, 18,  5, 25,\n",
      "       12,  0,  8,  0, 15, 15, 22, 16,  0, 12,  1, 14,  0]), array([15, 18, 15,  5, 14,  5,  1,  5, 15,  3, 12, 24, 15,  2, 14,  8,  1,\n",
      "        0, 19,  5,  0,  0,  6, 20, 14,  0,  5, 13,  0,  0,  0,  5,  0, 15,\n",
      "       13,  0,  9, 15, 19,  2, 12,  5,  5,  1,  0, 19, 14,  0, 19, 14,  0,\n",
      "       25, 19,  9,  9,  6,  0,  9, 15,  1,  9, 14,  1,  4]), array([ 3, 14, 14, 18,  3, 18, 14, 14, 14,  5,  0,  0, 19,  5,  9,  5, 14,\n",
      "       20,  9,  4,  9,  9,  0,  9,  6,  3, 19,  5,  1,  6,  5,  0, 15, 14,\n",
      "        1,  3,  3, 14,  9,  5,  5,  4, 13, 19, 20,  9,  9, 12, 20, 20, 14,\n",
      "        0,  8, 20, 14,  0, 18,  5, 14,  3, 14, 25, 12,  5]), array([ 1, 13,  1,  9,  5,  0,  7,  7,  1,  0, 11, 19, 19,  5, 26,  0,  0,\n",
      "        8,  7,  0, 14,  3, 20,  3, 21, 15, 20,  0, 20, 15, 19,  7, 18,  5,\n",
      "       18,  8,  1,  0, 19, 12,  1,  0,  0,  5,  8, 15, 14,  9,  0, 18,  5,\n",
      "       19,  1, 15,  9, 20,  9, 23,  5,  3,  5,  0,  0, 22]), array([20,  5, 12,  5, 19,  8, 21,  5, 12,  4, 14,  5,  0, 14,  5,  6,  9,\n",
      "        5, 14,  3,  0,  5,  8, 11, 19, 13,  0,  9, 20, 18, 15, 18,  9,  0,\n",
      "        3,  1, 12,  7,  0,  9, 19, 21,  5,  4,  5, 14,  5, 14,  4,  1,  8,\n",
      "       20, 18,  0, 20,  8,  3,  0, 14, 18,  0, 15,  2,  9]), array([ 5, 14,  0, 19, 19,  0,  1, 18,  0, 21, 15, 22,  3,  0,  0,  9, 14,\n",
      "        0, 19,  1,  4,  0,  5,  0,  9, 16, 15, 20,  5, 13, 20, 15,  7, 14,\n",
      "        8, 18,  0, 13,  6,  5, 20, 16, 24,  0,  0,  0,  0, 21,  1, 20, 18,\n",
      "        9, 13, 20,  9,  5, 11, 15, 20,  5,  5, 20, 21,  3])]\n",
      "['e social rel', 'nts failed t', ' park photog', 's index sacr', 's of castile', ' provided a ', 'age among je', 'rs in decemb', ' media and f', 'uring the on', 'own manufact', 'ven a widebo', 'covering som', ' one of the ', ' single acts', 'irst card fr', 'n jersey and', ' poverty and', 's of humanit', 'ause so aqui', 'denaturaliza', ' formation s', 'e input usua', ' to pull him', 'ion inabilit', 'plete an ope', 'of the mista', 't fort des m', 'empts by his', 'mats for mai', 'teric christ', 'owing popula', 'ginal docume', 'nine eight z', 'h eight list', 'racter lieut', ' mechanics a', 'm comparison', 'fundamental ', 'eve the conf', 't not parlia', 'pon by histo', 'xample rlc c', ' on the whol', ' official la', ' at this poi', ' three two o', 'ux enterpris', 'aily college', 'tion camp le', 'ru wished th', 'iff from fla', 'man s sydney', 'to begin neg', 'iatives the ', 'ese authors ', 'ky ricardo t', 'of mathemati', 't of arm is ', 'edited progr', 'external lin', 'ther state m', 'uddhism espe', 'ces possible']\n",
      "[' an']\n",
      "['nar']\n",
      "[1]\n",
      "[379]\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def batches2IDs(batches):\n",
    "    \"\"\"Convert a sequence of batches into the ids using char2id helper.\n",
    "    \"\"\"\n",
    "    l = [ np.array( [ char2id(x) for x in characters(b) ] ) for b in batches ]\n",
    "    return l\n",
    "\n",
    "def builddictbigram():\n",
    "    d = {}\n",
    "    n = 0\n",
    "    for i in range(vocabulary_size):\n",
    "        for j in range(vocabulary_size):\n",
    "            d[(i,j)] = n\n",
    "            n+=1\n",
    "    return d\n",
    "\n",
    "dictbigram = builddictbigram()\n",
    "# print(dictbigram)\n",
    "\n",
    "def ids2bigrams(l0, l1):\n",
    "    return np.array( [ dictbigram[x0, x1] for x0, x1 in zip(l0, l1) ] )\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings+1)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1+1)\n",
    "\n",
    "bx = train_batches.next()\n",
    "bids = batches2IDs(bx)\n",
    "print('train is list of unrollings+1, each a np.array(64,27) of a batch of one-hot encodings')\n",
    "print('train batch:', len(bx), 'array of', bx[0].shape)\n",
    "print('to-id is  list of unrollings+1, each a np.array(64) of a batch of id-embeddings')\n",
    "print('to_id batch:', len(bids), 'array of', bids[0].shape)\n",
    "print('to bigrams', ids2bigrams(bids[0],bids[1]))\n",
    "\n",
    "print(batches2IDs(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "\n",
    "# print(batches2string(train_batches.next()))\n",
    "v1 = valid_batches.next()\n",
    "v2 = valid_batches.next()\n",
    "print(batches2string(v1))\n",
    "print(batches2string(v2))\n",
    "print(ids2bigrams(batches2IDs(v1)[0],batches2IDs(v1)[1]))\n",
    "print(ids2bigrams(batches2IDs(v2)[0],batches2IDs(v2)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** PROBLEM 1 DONE IN 6_lstm **\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Aproach:\n",
    "\n",
    "1) Identify the observation from the label. ++\n",
    "\n",
    "2) Unrolling are necesary for the LSTM cells. ++\n",
    "\n",
    "3) Split train from labels at feed. ++\n",
    "\n",
    "4) Generate num_unrollings + 2 batches.\n",
    "\n",
    "5) first is bigram0, second is bigram1, third is label.\n",
    "\n",
    "6) Train over bigram2ID(bigram0, bigram1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inputs: list of 10 (64,)\n",
      "train_labels: list of 10 (64,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # all in 3 tensors.\n",
    "    # iria lo siguiente. no para una letra.\n",
    "    #ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, vocabulary_size], -0.1, 0.1))\n",
    "    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size**2, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        # lookup instead of matmul. they are equivalent in this context.\n",
    "        all_gates_state = tf.nn.embedding_lookup(ifcox, i) + tf.matmul(o, ifcom) + ifcob\n",
    "        #all_gates_state = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2*num_nodes])\n",
    "        update = all_gates_state[:, 2*num_nodes: 3*num_nodes]\n",
    "        output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    #train_data = list()\n",
    "    #for _ in range(num_unrollings + 1):\n",
    "    #    train_data.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "    train_inputs = list()\n",
    "    train_labels = list()\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "        train_labels.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "    # train_inputs = train_data[:num_unrollings]\n",
    "    # train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    # print('train_data: list of', len(train_data), train_data[0].get_shape())\n",
    "    print('train_inputs: list of', len(train_inputs), train_inputs[0].get_shape())\n",
    "    print('train_labels: list of', len(train_inputs), train_labels[0].get_shape())\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        # use of sparse_softmax because it accepts int64s\n",
    "        sofmacs = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        # sofmacs = tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        loss = tf.reduce_mean(sofmacs)\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    \n",
    "    # idem as train_data\n",
    "    sample_input = tf.placeholder(tf.int64, shape = [1])    \n",
    "    # sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294736 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      "rropyyrsa mv a toe eeca ntetl aas renavdwnrtwhnrrn pi nrc  tonfoxefnxw rgghpqf u\n",
      "pke t ks emitrdfc loishajsmnufqfdoqeotwat  nhfcntlogl rrhhrqdconeqae muvtwbqeezh\n",
      "yo rooanaasey ixkirliglj yx aeiactnalaxfihxe ukaninoaptanfsnfiweqriiz hlrois npd\n",
      "yvyooolsd zzclyryzkrzeu ltuosymq iy  dc y ojntrwetunzroa asekdkfixvt sgh  fepahp\n",
      "yvtnu sd ereawhieinttg hlhon  azat zb w  nbq tdfrktnnliwr ahtt zjfnftqaowhk naln\n",
      "================================================================================\n",
      "Validation set perplexity: 19.69\n",
      "Average loss at step 100: 2.782298 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.79\n",
      "Validation set perplexity: 12.46\n",
      "Average loss at step 200: 2.387638 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.84\n",
      "Validation set perplexity: 10.82\n",
      "Average loss at step 300: 2.204461 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.72\n",
      "Validation set perplexity: 9.89\n",
      "Average loss at step 400: 2.071413 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 9.30\n",
      "Average loss at step 500: 2.029601 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 9.40\n",
      "Average loss at step 600: 1.981530 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 10.27\n",
      "Average loss at step 700: 1.953869 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 8.95\n",
      "Average loss at step 800: 1.913486 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 9.04\n",
      "Average loss at step 900: 1.883886 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 9.27\n",
      "Average loss at step 1000: 1.854009 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "================================================================================\n",
      "mxition of the ung tradi incereniture fad the hafpire of the econter ip the seve\n",
      "uch not zero zero five been a cloule he comdia scre bctive midative fromment of \n",
      "cging wore for me a thide wigh wan ding on the mosturena he the amter and rical \n",
      "sjaturi unt the sam piuent with resitier lighe s are the uterm citlips for in on\n",
      "ob mated in opqtedery to s the expcs itretorder in three two kight governem by u\n",
      "================================================================================\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 1100: 1.842028 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 1200: 1.820970 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 8.53\n",
      "Average loss at step 1300: 1.841294 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 1400: 1.831923 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 1500: 1.823238 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 1600: 1.802029 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 1700: 1.794894 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 8.13\n",
      "Average loss at step 1800: 1.788309 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 1900: 1.783545 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 2000: 1.782229 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "================================================================================\n",
      "hp thingp five shokeir puop and be seven makk now calied composid a franceld ext\n",
      "aat inclultic can that i prited a centrome teat  is trise climut in a maggo conb\n",
      "nm libered annicrot a the one three zero two nine six edlonaled pasting a ferf b\n",
      "va nonccetner three two z y mandome by highat of mislolin timetre off the genavi\n",
      "w wys the is as in n the flurricatly a gor three four four nine six heo gage bag\n",
      "================================================================================\n",
      "Validation set perplexity: 8.41\n",
      "Average loss at step 2100: 1.766569 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 2200: 1.740454 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 8.31\n",
      "Average loss at step 2300: 1.737619 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 8.31\n",
      "Average loss at step 2400: 1.733633 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 9.17\n",
      "Average loss at step 2500: 1.755784 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 2600: 1.747539 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 8.27\n",
      "Average loss at step 2700: 1.733567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 2800: 1.746556 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 2900: 1.734159 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 3000: 1.716317 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "ce several can klish action of julitages gat ovcrodintembouad to knam very visio\n",
      "d one nine eight one eight undor fairs poirs the not phe dnestivation treer seci\n",
      "qj redic if sonlower of the sew debipreqes vicive deeds as the each presoned wor\n",
      "lbers be contatistent idirs to his acpnorimational of the two four one seven nin\n",
      "hmets procdure contsons at the ball a again two five two one one mile mosfrarole\n",
      "================================================================================\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 3100: 1.702379 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 3200: 1.681894 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 8.14\n",
      "Average loss at step 3300: 1.706236 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 3400: 1.688319 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 3500: 1.711365 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 8.12\n",
      "Average loss at step 3600: 1.707194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 3700: 1.691562 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 3800: 1.696402 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 3900: 1.681508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 8.06\n",
      "Average loss at step 4000: 1.673673 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "zxepoiving p noversion traded is man may same bassan and the serviciely round th\n",
      "vferenborous which borname depreders troem serves doom area revoluidy takes evgl\n",
      "egape nume to kno tota whethen cuo also eterneture of tell accersy when sit spec\n",
      "kosheir cipii minhostage one zero th s beenstanow the such the day work rranger \n",
      "umtal gaelism b one eight seven one jew time micution lonage an americal consire\n",
      "================================================================================\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 4100: 1.684618 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 4200: 1.682595 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 4300: 1.682814 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 4400: 1.651951 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 4500: 1.656714 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 4600: 1.670809 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 4700: 1.650535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 4800: 1.653270 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 4900: 1.650917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 5000: 1.653498 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "================================================================================\n",
      "rhing to can but exis larek and metror afaile websities mendaritie two zero one \n",
      "alaminensel the bbc by mink heaphicid by explains godring there cates bush warn \n",
      "r persional d carcies but rink has war the express in nine do its matal bire of \n",
      "am novelty of john westernal servicifical record is messictor been the leries in\n",
      "hc of simal change in the us fedrumme boyed the the meral is more is book worldi\n",
      "================================================================================\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 5100: 1.643946 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 5200: 1.613085 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 5300: 1.625638 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 5400: 1.649408 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 5500: 1.626968 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 5600: 1.627876 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 5700: 1.639452 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 5800: 1.657631 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 5900: 1.620762 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 6000: 1.606976 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "zustrienna troaken emerge own past zero zero s thas routas used of from these nu\n",
      "tne aid v one nine to bell was rime usually to jek wienties which the in one zer\n",
      "qhine out over no kall one eight s five three zero to evidengue the state usess \n",
      "uh creates culying marilole vareastical irst by dunes from he album on one five \n",
      "lvers l geoplementa under moving of a base about the tew one eighc six news zero\n",
      "================================================================================\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 6100: 1.642004 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 6200: 1.647238 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 6300: 1.632937 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 6400: 1.646384 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 6500: 1.671775 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 6600: 1.614284 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 6700: 1.592856 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 6800: 1.641883 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 6900: 1.630073 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 7000: 1.614408 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "il dune of had designing argle are many bandas the has fourman at the with perma\n",
      "nz the down of timulater corling texplessed leadnel defer one commedican ke boon\n",
      "dc velow as mat seturies but churred model evidun the acrust to empirocing could\n",
      " power suere of plantical consign date originally a depbms is asmainly the progr\n",
      "vzmony geopbies in the united anomass day widertybegs enxambers unclogice of be \n",
      "================================================================================\n",
      "Validation set perplexity: 7.38\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        # transform input into IDs\n",
    "        batchesIDs = batches2IDs( batches )\n",
    "        # print(batchesIDs[0])\n",
    "        # print(batchesIDs[1])\n",
    "        # print(batchesIDs[2])\n",
    "        # print(ids2bigrams(batchesIDs[0],batchesIDs[1]))\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = ids2bigrams(batchesIDs[i], batchesIDs[i+1])\n",
    "            feed_dict[train_labels[i]] = batchesIDs[i+2]\n",
    "        # print(feed_dict)\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed0 = sample(random_distribution())\n",
    "                    sentence = characters(feed0)[0]\n",
    "                    feed1 = sample(random_distribution())\n",
    "                    sentence += characters(feed1)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(78):\n",
    "                        # transform input into IDs\n",
    "                        # batches2IDs accepts list. we need to wrap and unwrap.\n",
    "                        bIDs = batches2IDs( [feed0, feed1] )\n",
    "                        feedID = ids2bigrams(bIDs[0], bIDs[1])\n",
    "                        prediction = sample_prediction.eval({sample_input: feedID})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        feed0 = feed1\n",
    "                        feed1 = feed\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                # transform input into IDs\n",
    "                bIDs = batches2IDs(b)\n",
    "                predictions = sample_prediction.eval({sample_input: ids2bigrams(bIDs[0],bIDs[1])})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
