{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train is list of unrollings+1, each a np.array(64,27) of a batch of one-hot encodings\n",
      "train batch: 12 array of (64, 27)\n",
      "to-id is  list of unrollings+1, each a np.array(64) of a batch of id-embeddings\n",
      "to_id batch: 12 array of (64,)\n",
      "to bigrams [419 629 336   1 352 221 675  52 549 360 383 221 135 137 405 680 423  27\n",
      "  20  45 263   1 549 133 162  47 135 149  40 508 258 405  27 197 257 501\n",
      "  82  46   4 366 540 567 135 405 411 513 309 153 640 155 548 155  19 262\n",
      " 549 108 548 149 167 129 558  47  43 522]\n",
      "[array([19, 18,  5,  0,  1,  8,  7, 15,  8, 15,  8,  5, 20, 15, 15,  5,  3,\n",
      "        9,  7,  0, 12,  5,  0,  8, 18, 20, 20, 13,  0, 20, 19,  5,  0,  0,\n",
      "        0,  8, 19,  5, 12, 15,  0,  4, 14, 20, 14,  0, 19, 21, 20,  9, 19,\n",
      "       12, 20, 18,  9,  0, 19,  3,  9, 13,  3,  0,  5,  5]), array([20, 25, 19, 13,  3,  1,  9, 18,  5, 15,  5, 22,  8,  2,  0,  9,  1,\n",
      "       20,  8, 21, 15, 12, 19,  9, 21,  1,  8,  0,  2,  1, 21,  0, 15,  8,\n",
      "        5,  5,  0,  0,  0, 18, 15,  9,  7, 25,  7,  3, 19, 24,  8,  0, 15,\n",
      "        1, 23,  0, 20, 13, 11,  0, 18,  0,  5, 20, 22,  0]), array([19,  0,  0, 15,  1, 18,  3,  0,  0, 11, 18,  5,  0,  1, 18, 22, 14,\n",
      "        9, 20, 14, 19, 12,  9, 13,  7, 11,  5, 20,  1, 14,  3, 15,  6,  9,\n",
      "        9,  0,  3, 20,  1, 13, 18, 19,  0, 16, 21, 15,  0,  0,  5,  3,  3,\n",
      "       20, 15,  8,  9, 15,  5, 15,  0,  1, 14,  8, 15, 19]), array([ 0,  7, 14, 14,  0,  4,  1, 16, 14,  0,  0, 14,  1,  2,  5,  5, 20,\n",
      "        3,  0,  3, 20, 21, 26,  0, 19,  5,  0, 15, 18,  4,  8, 14,  0, 22,\n",
      "        7, 12, 12,  8, 14, 15,  0,  1, 19,  5,  1, 13, 15, 19,  0, 15,  9,\n",
      "        9, 18,  9,  3, 19, 18, 22,  3,  3, 20,  1, 20, 21]), array([ 1, 15,  1,  1, 16,  0, 12,  1,  1, 16, 23,  0,  0, 12,  3,  4,  0,\n",
      "        0,  9,  1,  0, 12,  5,  1,  0,  0, 16,  0, 18,  1,  0,  0, 20,  5,\n",
      "        8,  5,  1,  5,  1, 14,  1,  7, 25, 19,  7, 13, 14, 21,  6, 14,  5,\n",
      "       22, 11, 18,  1, 20,  4,  5, 15, 14,  5, 14,  9,  3]), array([ 4, 22, 20, 19, 18,  2,  0, 19, 20, 12,  5, 19,  7, 25, 15,  0, 20,\n",
      "       15, 14, 21,  1,  1,  0,  0,  3, 20, 18, 14,  5, 18,  1, 20,  8, 18,\n",
      "       20,  1, 19,  0, 12, 19, 20, 18, 19,  0,  5,  9,  5, 19,  9,  3, 20,\n",
      "        5, 19, 15, 12,  0, 15, 18, 13, 13, 18,  0, 15,  8]), array([22,  5,  9, 20,  9,  1, 12, 19,  9,  1, 12,  9, 12,  0,  7, 20,  8,\n",
      "        6,  0, 19, 19, 18, 15, 19, 15, 15,  9,  1,  4,  4, 19,  8,  5,  0,\n",
      "        0,  4, 19, 14, 25,  0,  0,  5, 20,  2, 19, 19,  0,  5, 18,  5, 25,\n",
      "       12,  0,  8,  0, 15, 15, 22, 16,  0, 12,  1, 14,  0]), array([15, 18, 15,  5, 14,  5,  1,  5, 15,  3, 12, 24, 15,  2, 14,  8,  1,\n",
      "        0, 19,  5,  0,  0,  6, 20, 14,  0,  5, 13,  0,  0,  0,  5,  0, 15,\n",
      "       13,  0,  9, 15, 19,  2, 12,  5,  5,  1,  0, 19, 14,  0, 19, 14,  0,\n",
      "       25, 19,  9,  9,  6,  0,  9, 15,  1,  9, 14,  1,  4]), array([ 3, 14, 14, 18,  3, 18, 14, 14, 14,  5,  0,  0, 19,  5,  9,  5, 14,\n",
      "       20,  9,  4,  9,  9,  0,  9,  6,  3, 19,  5,  1,  6,  5,  0, 15, 14,\n",
      "        1,  3,  3, 14,  9,  5,  5,  4, 13, 19, 20,  9,  9, 12, 20, 20, 14,\n",
      "        0,  8, 20, 14,  0, 18,  5, 14,  3, 14, 25, 12,  5]), array([ 1, 13,  1,  9,  5,  0,  7,  7,  1,  0, 11, 19, 19,  5, 26,  0,  0,\n",
      "        8,  7,  0, 14,  3, 20,  3, 21, 15, 20,  0, 20, 15, 19,  7, 18,  5,\n",
      "       18,  8,  1,  0, 19, 12,  1,  0,  0,  5,  8, 15, 14,  9,  0, 18,  5,\n",
      "       19,  1, 15,  9, 20,  9, 23,  5,  3,  5,  0,  0, 22]), array([20,  5, 12,  5, 19,  8, 21,  5, 12,  4, 14,  5,  0, 14,  5,  6,  9,\n",
      "        5, 14,  3,  0,  5,  8, 11, 19, 13,  0,  9, 20, 18, 15, 18,  9,  0,\n",
      "        3,  1, 12,  7,  0,  9, 19, 21,  5,  4,  5, 14,  5, 14,  4,  1,  8,\n",
      "       20, 18,  0, 20,  8,  3,  0, 14, 18,  0, 15,  2,  9]), array([ 5, 14,  0, 19, 19,  0,  1, 18,  0, 21, 15, 22,  3,  0,  0,  9, 14,\n",
      "        0, 19,  1,  4,  0,  5,  0,  9, 16, 15, 20,  5, 13, 20, 15,  7, 14,\n",
      "        8, 18,  0, 13,  6,  5, 20, 16, 24,  0,  0,  0,  0, 21,  1, 20, 18,\n",
      "        9, 13, 20,  9,  5, 11, 15, 20,  5,  5, 20, 21,  3])]\n",
      "['e social rel', 'nts failed t', ' park photog', 's index sacr', 's of castile', ' provided a ', 'age among je', 'rs in decemb', ' media and f', 'uring the on', 'own manufact', 'ven a widebo', 'covering som', ' one of the ', ' single acts', 'irst card fr', 'n jersey and', ' poverty and', 's of humanit', 'ause so aqui', 'denaturaliza', ' formation s', 'e input usua', ' to pull him', 'ion inabilit', 'plete an ope', 'of the mista', 't fort des m', 'empts by his', 'mats for mai', 'teric christ', 'owing popula', 'ginal docume', 'nine eight z', 'h eight list', 'racter lieut', ' mechanics a', 'm comparison', 'fundamental ', 'eve the conf', 't not parlia', 'pon by histo', 'xample rlc c', ' on the whol', ' official la', ' at this poi', ' three two o', 'ux enterpris', 'aily college', 'tion camp le', 'ru wished th', 'iff from fla', 'man s sydney', 'to begin neg', 'iatives the ', 'ese authors ', 'ky ricardo t', 'of mathemati', 't of arm is ', 'edited progr', 'external lin', 'ther state m', 'uddhism espe', 'ces possible']\n",
      "[' an']\n",
      "['nar']\n",
      "[1]\n",
      "[379]\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def batches2IDs(batches):\n",
    "    \"\"\"Convert a sequence of batches into the ids using char2id helper.\n",
    "    \"\"\"\n",
    "    l = [ np.array( [ char2id(x) for x in characters(b) ] ) for b in batches ]\n",
    "    return l\n",
    "\n",
    "def builddictbigram():\n",
    "    d = {}\n",
    "    n = 0\n",
    "    for i in range(vocabulary_size):\n",
    "        for j in range(vocabulary_size):\n",
    "            d[(i,j)] = n\n",
    "            n+=1\n",
    "    return d\n",
    "\n",
    "dictbigram = builddictbigram()\n",
    "# print(dictbigram)\n",
    "\n",
    "def ids2bigrams(l0, l1):\n",
    "    return np.array( [ dictbigram[x0, x1] for x0, x1 in zip(l0, l1) ] )\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings+1)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1+1)\n",
    "\n",
    "bx = train_batches.next()\n",
    "bids = batches2IDs(bx)\n",
    "print('train is list of unrollings+1, each a np.array(64,27) of a batch of one-hot encodings')\n",
    "print('train batch:', len(bx), 'array of', bx[0].shape)\n",
    "print('to-id is  list of unrollings+1, each a np.array(64) of a batch of id-embeddings')\n",
    "print('to_id batch:', len(bids), 'array of', bids[0].shape)\n",
    "print('to bigrams', ids2bigrams(bids[0],bids[1]))\n",
    "\n",
    "print(batches2IDs(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "\n",
    "# print(batches2string(train_batches.next()))\n",
    "v1 = valid_batches.next()\n",
    "v2 = valid_batches.next()\n",
    "print(batches2string(v1))\n",
    "print(batches2string(v2))\n",
    "print(ids2bigrams(batches2IDs(v1)[0],batches2IDs(v1)[1]))\n",
    "print(ids2bigrams(batches2IDs(v2)[0],batches2IDs(v2)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** PROBLEM 1 DONE IN 6_lstm **\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Aproach:\n",
    "\n",
    "1) Identify the observation from the label. ++\n",
    "\n",
    "2) Unrolling are necesary for the LSTM cells. ++\n",
    "\n",
    "3) Split train from labels at feed. ++\n",
    "\n",
    "4) Generate num_unrollings + 2 batches.\n",
    "\n",
    "5) first is bigram0, second is bigram1, third is label.\n",
    "\n",
    "6) Train over bigram2ID(bigram0, bigram1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inputs: list of 10 (64,)\n",
      "train_labels: list of 10 (64,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # all in 3 tensors.\n",
    "    # iria lo siguiente. no para una letra.\n",
    "    #ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, vocabulary_size], -0.1, 0.1))\n",
    "    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size**2, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        # lookup instead of matmul. they are equivalent in this context.\n",
    "        all_gates_state = tf.nn.embedding_lookup(ifcox, i) + tf.matmul(o, ifcom) + ifcob\n",
    "        #all_gates_state = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2*num_nodes])\n",
    "        update = all_gates_state[:, 2*num_nodes: 3*num_nodes]\n",
    "        output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    #train_data = list()\n",
    "    #for _ in range(num_unrollings + 1):\n",
    "    #    train_data.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "    train_inputs = list()\n",
    "    train_labels = list()\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "        train_labels.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "    # train_inputs = train_data[:num_unrollings]\n",
    "    # train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    # print('train_data: list of', len(train_data), train_data[0].get_shape())\n",
    "    print('train_inputs: list of', len(train_inputs), train_inputs[0].get_shape())\n",
    "    print('train_labels: list of', len(train_inputs), train_labels[0].get_shape())\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        # use of sparse_softmax because it accepts int64s\n",
    "        sofmacs = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        # sofmacs = tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        loss = tf.reduce_mean(sofmacs)\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    \n",
    "    # idem as train_data\n",
    "    sample_input = tf.placeholder(tf.int64, shape = [1])    \n",
    "    # sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297880 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "Validation set perplexity: 19.28\n",
      "Average loss at step 100: 2.772757 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.68\n",
      "Validation set perplexity: 15.20\n",
      "Average loss at step 200: 2.368306 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.76\n",
      "Validation set perplexity: 10.07\n",
      "Average loss at step 300: 2.182896 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.26\n",
      "Validation set perplexity: 9.23\n",
      "Average loss at step 400: 2.069412 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 8.99\n",
      "Average loss at step 500: 2.033997 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.28\n",
      "Validation set perplexity: 9.13\n",
      "Average loss at step 600: 1.972021 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 700: 1.923243 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 8.97\n",
      "Average loss at step 800: 1.913704 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 8.81\n",
      "Average loss at step 900: 1.916319 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 1000: 1.859945 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "================================================================================\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 1100: 1.841529 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 8.83\n",
      "Average loss at step 1200: 1.818048 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 1300: 1.836163 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 1400: 1.825775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 1500: 1.823581 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 8.33\n",
      "Average loss at step 1600: 1.776333 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 1700: 1.743316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 1800: 1.769220 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 1900: 1.774293 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 2000: 1.753280 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 2100: 1.728858 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 2200: 1.742030 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 2300: 1.736620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 2400: 1.748481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 2500: 1.724100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 2600: 1.727005 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 2700: 1.719714 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 2800: 1.702012 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 2900: 1.714997 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 3000: 1.712847 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 3100: 1.733665 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 3200: 1.718871 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 3300: 1.723825 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 3400: 1.711054 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 3500: 1.706806 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 3600: 1.696093 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 3700: 1.697892 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 3800: 1.689795 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 3900: 1.678958 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 4000: 1.669124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 4100: 1.667221 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 4200: 1.681963 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 4300: 1.674549 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 4400: 1.685121 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 4500: 1.674592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 4600: 1.654274 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 4700: 1.672376 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 4800: 1.648517 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 4900: 1.640456 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 5000: 1.629772 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "================================================================================\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 5100: 1.625870 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 5200: 1.624884 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 5300: 1.632403 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 5400: 1.617744 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 5500: 1.603917 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 5600: 1.599830 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 5700: 1.602997 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 5800: 1.597468 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 5900: 1.612107 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 6000: 1.641362 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "================================================================================\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 6100: 1.645298 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 6200: 1.644057 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 6300: 1.628616 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 6400: 1.636444 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 6500: 1.635757 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 6600: 1.621435 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 6700: 1.638614 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 6800: 1.653043 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 6900: 1.610211 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 7000: 1.617345 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "Validation set perplexity: 7.54\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        # transform input into IDs\n",
    "        batchesIDs = batches2IDs( batches )\n",
    "        # print(batchesIDs[0])\n",
    "        # print(batchesIDs[1])\n",
    "        # print(batchesIDs[2])\n",
    "        # print(ids2bigrams(batchesIDs[0],batchesIDs[1]))\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = ids2bigrams(batchesIDs[i], batchesIDs[i+1])\n",
    "            feed_dict[train_labels[i]] = batchesIDs[i+2]\n",
    "        # print(feed_dict)\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                #print('=' * 80)\n",
    "                #for _ in range(5):\n",
    "                #    feed = sample(random_distribution())\n",
    "                #    sentence = characters(feed)[0]\n",
    "                #    reset_sample_state.run()\n",
    "                #    for _ in range(79):\n",
    "                #        # transform input into IDs\n",
    "                #        # batches2IDs accepts list. we need to wrap and unwrap.\n",
    "                #        feedID = ids2bigrams(batches2IDs([ feed ])[0],batches2IDs([ feed ])[1])\n",
    "                #        prediction = sample_prediction.eval({sample_input: feedID})\n",
    "                #        feed = sample(prediction)\n",
    "                #        sentence += characters(feed)[0]\n",
    "                #    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                # transform input into IDs\n",
    "                bIDs = batches2IDs(b)\n",
    "                predictions = sample_prediction.eval({sample_input: ids2bigrams(bIDs[0],bIDs[1])})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
