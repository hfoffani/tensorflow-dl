{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([15, 23, 12,  0, 13,  8, 25,  1, 20, 13, 14,  8,  5,  5, 15, 25, 15,\n",
      "        1,  0,  1,  9,  0, 20,  4,  6,  1,  5,  5,  1, 18,  9, 15,  1,  7,\n",
      "        9, 18,  3,  1,  0, 13, 20, 21,  5, 15, 15, 19, 11,  5, 23,  5, 20,\n",
      "        5,  0,  9, 20,  4, 20,  5,  6,  4, 20,  1,  1, 19]), array([14,  8, 12,  1,  1,  5,  0, 25,  9,  9,  5,  5,  0,  2,  0,  5, 18,\n",
      "        0, 20, 18, 20,  1,  9, 25,  0, 20,  0, 14, 13, 22, 15,  0,  0,  8,\n",
      "       14, 15,  1, 19,  4, 15,  0,  0,  0,  0,  6,  0, 12, 18, 19, 20,  8,\n",
      "       20, 19, 19,  9,  0,  8, 14,  5, 21, 18, 20, 16,  9]), array([19,  5,  5,  2, 18, 12,  1,  0, 15,  7, 23,  0, 12,  5,  2, 18,  5,\n",
      "        6, 23,  9, 25, 14, 15,  0,  3,  0,  3, 20, 16,  5, 21,  3,  4,  0,\n",
      "        5, 19, 12, 20,  9, 19, 19,  9, 15,  5,  0, 20,  1, 16,  0,  0,  5,\n",
      "        3,  8,  5, 14, 14,  0,  3, 14,  1,  5,  9, 16,  0]), array([ 0, 14, 18,  2, 18,  0, 14, 15, 14, 18,  0,  2,  9, 18,  5,  0,  0,\n",
      "        9, 15, 19,  0,  4, 14, 20,  5,  9, 15,  0,  1, 18, 19,  1, 21,  1,\n",
      "        0, 19,  0,  0, 13, 20,  0, 19, 19,  9,  9,  8,  8, 18,  2,  9,  0,\n",
      "        8,  1,  4,  7,  5, 18, 25, 19, 20,  5, 15,  5,  8]), array([ 1,  0,  9,  5,  9,  1,  4, 16,  0,  1, 25, 15, 19,  0,  0, 23, 19,\n",
      "        5,  0, 20,  3,  0,  0, 15, 18, 20, 14, 20,  9,  0,  0, 16, 16, 14,\n",
      "       10,  0, 20,  9,  5,  0, 19,  0,  3,  7, 20,  5, 15,  9,  5, 14,  6,\n",
      "       25, 18,  0,  0, 15,  9,  3,  5,  9, 20, 14,  1,  1]), array([14, 13,  1, 25,  5, 14,  0,  5,  6, 20, 15,  5, 20,  8, 13,  8,  9,\n",
      "       18, 19, 15,  1,  9, 15,  0, 20,  0, 22, 15,  7, 19, 20,  9, 12, 14,\n",
      "        1, 26,  8, 14, 14,  8, 21, 19,  9,  8,  1,  0, 13, 19,  3,  0,  1,\n",
      "        0, 13,  5,  9,  0, 19, 12,  0, 14,  0, 19, 12, 22]), array([ 1,  9,  0, 19,  4,  4, 12, 14, 18,  9, 18,  9,  5,  1,  1, 15,  7,\n",
      "        3,  9, 20, 14, 14,  6, 16,  1, 23,  9, 12, 14,  9,  5, 20,  9,  0,\n",
      "       14,  5,  5, 19, 19, 15, 16, 20, 12, 20, 12, 20,  1,  5, 15,  1,  2,\n",
      "       20,  1, 13, 14, 12, 11, 15, 20,  7,  7,  0,  0,  5]), array([18, 12,  1,  0,  0,  0,  9,  5, 15, 15, 11, 14,  4, 19,  4,  0, 14,\n",
      "        5, 24, 12,  0, 20,  0,  1,  9,  9, 14,  4,  0,  4, 24,  1,  3,  5,\n",
      "       21, 18, 15, 20,  9, 12, 16,  9, 12,  0, 25, 15,  0,  0, 13,  0,  9,\n",
      "       15, 14, 16,  0,  1, 25, 16,  8,  0, 18, 13, 15,  0]), array([ 3,  9, 18,  1, 21, 18, 20,  4, 13, 14,  0,  7,  0,  0,  5, 18,  9,\n",
      "        0,  0,  5,  2, 18, 20, 19, 14, 12,  3,  0,  1,  5, 20, 12,  1, 19,\n",
      "        1, 15, 18,  1, 15, 25, 15, 12,  1, 19,  0, 23, 16, 12,  5, 14,  1,\n",
      "        0,  0,  5, 16, 20,  0,  5,  5,  6,  9, 15,  6, 13]), array([ 8, 20,  3, 14, 18,  9, 21,  0,  0,  0, 15,  0, 23, 16,  0,  5,  6,\n",
      "        3,  5,  0,  5,  1,  8, 19,  0, 12,  5,  8, 14,  0, 19,  9, 20,  0,\n",
      "       18,  0,  9, 14, 14,  0, 18, 12, 20, 21, 12,  5, 18,  9, 19,  1, 14,\n",
      "       18, 14, 18, 15,  9, 18,  4,  0, 18,  4, 18,  0,  1]), array([ 9,  1,  8,  4, 18,  3, 18,  6, 20, 20, 20, 19,  9, 18, 20,  3,  9,\n",
      "       18,  9, 19,  0,  3,  5,  0,  4,  0,  0,  9,  4, 19,  0, 26,  5,  4,\n",
      "       25, 20,  5,  3,  1, 13, 20,  0,  9,  2,  1, 18,  5, 14,  0, 26,  0,\n",
      "        5,  5, 15, 12, 14,  9,  9,  1, 15,  0,  5,  4,  4])]\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "['ate social ', 'ments faile', 'al park pho', 'ies index s', 'ess of cast', ' h provided', 'guage among', 'gers in dec', 'al media an', ' during the', 'known manuf', 'seven a wid', 's covering ', 'en one of t', 'ze single a', ' first card', ' in jersey ', 'he poverty ', 'gns of huma', ' cause so a', 'n denatural', 'ce formatio', 'the input u', 'ck to pull ', 'usion inabi', 'omplete an ', 't of the mi', ' it fort de', 'ttempts by ', 'ormats for ', 'soteric chr', 'growing pop', 'riginal doc', 'e nine eigh', 'rch eight l', 'haracter li', 'al mechanic', ' gm compari', 's fundament', 'lieve the c', 'ast not par', ' upon by hi', ' example rl', 'ed on the w', 'he official', 'on at this ', 'ne three tw', 'inux enterp', ' daily coll', 'ration camp', 'ehru wished', 'stiff from ', 'arman s syd', 'o to begin ', 'itiatives t', 'these autho', 'icky ricard', 'w of mathem', 'ent of arm ', 'credited pr', 'e external ', ' other stat', ' buddhism e', 'vices possi']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def batches2IDs(batches):\n",
    "    \"\"\"Convert a sequence of batches into the ids using char2id helper.\n",
    "    \"\"\"\n",
    "    l = [ np.array( [ char2id(x) for x in characters(b) ] ) for b in batches ]\n",
    "    return l\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2IDs(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** PROBLEM 1 DONE IN 6_lstm **\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aproach:\n",
    "\n",
    "1) Identify the observation from the label.\n",
    "\n",
    "2) Change char2id should receive 2 chars or a tuple.\n",
    "\n",
    "3) One-hot should receive 2 chars or a tuple.\n",
    "\n",
    "4) A wrapper over BatchGenerator or a new one?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # all in 3 tensors.\n",
    "    # iria lo siguiente. no para una letra.\n",
    "    #ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, vocabulary_size], -0.1, 0.1))\n",
    "    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        # lookup instead of matmul. they are equivalent in this context.\n",
    "        all_gates_state = tf.nn.embedding_lookup(ifcox, i) + tf.matmul(o, ifcom) + ifcob\n",
    "        #all_gates_state = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2*num_nodes])\n",
    "        update = all_gates_state[:, 2*num_nodes: 3*num_nodes]\n",
    "        output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "                # ids as input.\n",
    "                tf.placeholder(tf.int64, shape=[batch_size]))\n",
    "                # tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        # use of sparse_softmax because it accepts int64s\n",
    "        sofmacs = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        # sofmacs = tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        loss = tf.reduce_mean(sofmacs)\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    \n",
    "    # idem as train_data\n",
    "    sample_input = tf.placeholder(tf.int64, shape = [1])    \n",
    "    # sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296383 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "uloezgsbdrsmufuy  a grs dxriiaen z lnn sldejtwmn   oercg ez fpltarpk acehxlz ehd\n",
      "gspc  upgirynerx ohaiov euird srx ismhzmga drc dyymoij ken egtzmii  abjed tqzhgk\n",
      "gucxaungkshrsbnnq wecpvwlnayjhexfpdoeloivpzgzrgo kptj c  rnqcfhtxavnfduytpoy tga\n",
      "yidw  vknltcjrsi tv zxoeer ithnyq pmrauqepzmezl oazff m ehjnleum it eobehsu xoln\n",
      "zd kohlv earsmzfo  oaaate q jckqiquewza stactnyaeusoa  kslnwntltuehinjedmfyew ol\n",
      "================================================================================\n",
      "Validation set perplexity: 20.17\n",
      "Average loss at step 100: 2.602232 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.94\n",
      "Validation set perplexity: 11.63\n",
      "Average loss at step 200: 2.280975 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.51\n",
      "Validation set perplexity: 8.82\n",
      "Average loss at step 300: 2.138714 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.09\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 400: 2.019730 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 500: 1.942261 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 600: 1.919705 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 700: 1.882632 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 800: 1.864693 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 900: 1.823165 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 1000: 1.801126 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "================================================================================\n",
      "jet but ofs heakes liqs two nithersion somba arty tho ans antepricedite though e\n",
      "jory opelopy a the many notorkhd a frem throung e manulion lecaled fualitifes in\n",
      "biladic on castions or hranders engine woy some affanting gowers this even infau\n",
      "gtle by ahdpel stenties hane jutern which was in one five fims grougiaading thro\n",
      "ding the apivessent one nine ead trag bunie four gyanded by suulard forblary hol\n",
      "================================================================================\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1100: 1.767858 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1200: 1.764961 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1300: 1.736614 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1400: 1.740944 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1500: 1.755695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1600: 1.747351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1700: 1.737395 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1800: 1.723574 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1900: 1.706562 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 2000: 1.716665 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "================================================================================\n",
      "with a brider beciabiliz as one six five a dou clayd he and attem base two conti\n",
      "ter for zero one nine five one five one eight zero one nine six zero zero zero z\n",
      "e aurast of chil mihines cocontroc for eccemucte lieis from two zero zero zero j\n",
      "and about mista curuge quication an agailany a hud ax averal constituty and nech\n",
      "nes usuafal julhar a verile that that high seigitam ateos of the borl signs for \n",
      "================================================================================\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 2100: 1.715467 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2200: 1.702166 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2300: 1.707860 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2400: 1.672387 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2500: 1.665639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2600: 1.669314 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2700: 1.655441 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2800: 1.699261 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2900: 1.673728 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3000: 1.674645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "fes or a buri alabur tradita quters with closing plaze assoctan a and one nine t\n",
      "s two also ponyer figates however an in dome of traesativatics by the mark of si\n",
      "pines and in the officced proper noves natic sotu eld to ocneal provust octs bud\n",
      "balitame where commution compakes in the arpafi to aldeding some stapse orttlers\n",
      "printwing field brind languagaytern sabut oree nime two the subbilation wife int\n",
      "================================================================================\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3100: 1.676884 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3200: 1.670809 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3300: 1.657166 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3400: 1.631686 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3500: 1.631034 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3600: 1.630255 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3700: 1.632197 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3800: 1.649521 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3900: 1.636777 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 4000: 1.643683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "ver in bre dispe with a coling two nany act one zero two zero seven three four e\n",
      "gen langers some pown plait in one six one nine one six eight three one nine two\n",
      "s stated hajland first where larger was a combineillet an juriam michtoey ko reg\n",
      "que it definitating in the eight is to bwersan glatapliever mainic book to comme\n",
      "ts the korling unyolls tower mear two five zero lour spressey spect modern refer\n",
      "================================================================================\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4100: 1.623364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4200: 1.630035 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4300: 1.633353 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4400: 1.612103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4500: 1.607307 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4600: 1.632245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4700: 1.618548 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4800: 1.610683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4900: 1.589981 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 5000: 1.600698 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "rether emptreent herbeelbadrage from the rueb although reflect listly celturic w\n",
      "ustitiumanisticliagey there calopobort rechty of the scholveph lacroal unitifink\n",
      "vial birti new but sitzart services adomser awarn of bicastol used by nosable br\n",
      "dethire are of the five birtrcarity pume imocker ans redies to church basssor ba\n",
      "k licfagers and elemersms and defterated a red the roams primace hgw music the r\n",
      "================================================================================\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5100: 1.614666 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5200: 1.586023 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5300: 1.588422 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5400: 1.582098 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5500: 1.589025 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5600: 1.587170 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5700: 1.586998 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5800: 1.546430 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5900: 1.600441 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6000: 1.579288 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "asher in to was such anallerely stile grardi pot rathurio substend three one sev\n",
      "es chaivable imformation tripical one two spainallus whered yercors brurds focus\n",
      "whe nine the michip under forments appearatizes jome the midetever anthrespay it\n",
      "zenical of even childing the gainst but by the suck just formational concentroms\n",
      "decal casers thal and supborphine be trains calking clunchk eng while hopetious \n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6100: 1.574646 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6200: 1.576580 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6300: 1.581699 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6400: 1.589086 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6500: 1.590005 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6600: 1.558347 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6700: 1.562877 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6800: 1.598538 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6900: 1.571927 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 7000: 1.581299 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      " malings in the trade apprimatical mirtholown a between the implestina for comma\n",
      "cinial in is a like lood in give seven follo remarly as durings that a more for \n",
      "chuker syjuter due by the among hti of i the commegrat enemy and bakes to pie re\n",
      "d for seven wois ned bole and by land woull nown dilface currenter the three the\n",
      "b cathabition greem and not product to forled beginals and hass in and releed pr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        # transform input into IDs\n",
    "        batchesIDs = batches2IDs( batches )\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batchesIDs[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        # transform input into IDs\n",
    "                        # batches2IDs accepts list. we need to wrap and unwrap.\n",
    "                        feedID = batches2IDs([ feed ])[0]\n",
    "                        prediction = sample_prediction.eval({sample_input: feedID})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                # transform input into IDs\n",
    "                bIDs = batches2IDs(b)\n",
    "                predictions = sample_prediction.eval({sample_input: bIDs[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
