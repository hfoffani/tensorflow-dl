{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train is list of unrollings, each a np.array(64,27) of a batch of one-hot encodings\n",
      "train batch: 11 array of (64, 27)\n",
      "to-id is  list of unrollings, each a np.array(64) of a batch of id-embeddings\n",
      "to_id batch: 11 array of (64,)\n",
      "[array([ 9,  1,  8,  4, 18,  3, 18,  6, 20, 20, 20, 19,  9, 18, 20,  3,  9,\n",
      "       18,  9, 19,  0,  3,  5,  0,  4,  0,  0,  9,  4, 19,  0, 26,  5,  4,\n",
      "       25, 20,  5,  3,  1, 13, 20,  0,  9,  2,  1, 18,  5, 14,  0, 26,  0,\n",
      "        5,  5, 15, 12, 14,  9,  9,  1, 15,  0,  5,  4,  4]), array([19, 18,  5,  0,  1,  8,  7, 15,  8, 15,  8,  5, 20, 15, 15,  5,  3,\n",
      "        9,  7,  0, 12,  5,  0,  8, 18, 20, 20, 13,  0, 20, 19,  5,  0,  0,\n",
      "        0,  8, 19,  5, 12, 15,  0,  4, 14, 20, 14,  0, 19, 21, 20,  9, 19,\n",
      "       12, 20, 18,  9,  0, 19,  3,  9, 13,  3,  0,  5,  5]), array([20, 25, 19, 13,  3,  1,  9, 18,  5, 15,  5, 22,  8,  2,  0,  9,  1,\n",
      "       20,  8, 21, 15, 12, 19,  9, 21,  1,  8,  0,  2,  1, 21,  0, 15,  8,\n",
      "        5,  5,  0,  0,  0, 18, 15,  9,  7, 25,  7,  3, 19, 24,  8,  0, 15,\n",
      "        1, 23,  0, 20, 13, 11,  0, 18,  0,  5, 20, 22,  0]), array([19,  0,  0, 15,  1, 18,  3,  0,  0, 11, 18,  5,  0,  1, 18, 22, 14,\n",
      "        9, 20, 14, 19, 12,  9, 13,  7, 11,  5, 20,  1, 14,  3, 15,  6,  9,\n",
      "        9,  0,  3, 20,  1, 13, 18, 19,  0, 16, 21, 15,  0,  0,  5,  3,  3,\n",
      "       20, 15,  8,  9, 15,  5, 15,  0,  1, 14,  8, 15, 19]), array([ 0,  7, 14, 14,  0,  4,  1, 16, 14,  0,  0, 14,  1,  2,  5,  5, 20,\n",
      "        3,  0,  3, 20, 21, 26,  0, 19,  5,  0, 15, 18,  4,  8, 14,  0, 22,\n",
      "        7, 12, 12,  8, 14, 15,  0,  1, 19,  5,  1, 13, 15, 19,  0, 15,  9,\n",
      "        9, 18,  9,  3, 19, 18, 22,  3,  3, 20,  1, 20, 21]), array([ 1, 15,  1,  1, 16,  0, 12,  1,  1, 16, 23,  0,  0, 12,  3,  4,  0,\n",
      "        0,  9,  1,  0, 12,  5,  1,  0,  0, 16,  0, 18,  1,  0,  0, 20,  5,\n",
      "        8,  5,  1,  5,  1, 14,  1,  7, 25, 19,  7, 13, 14, 21,  6, 14,  5,\n",
      "       22, 11, 18,  1, 20,  4,  5, 15, 14,  5, 14,  9,  3]), array([ 4, 22, 20, 19, 18,  2,  0, 19, 20, 12,  5, 19,  7, 25, 15,  0, 20,\n",
      "       15, 14, 21,  1,  1,  0,  0,  3, 20, 18, 14,  5, 18,  1, 20,  8, 18,\n",
      "       20,  1, 19,  0, 12, 19, 20, 18, 19,  0,  5,  9,  5, 19,  9,  3, 20,\n",
      "        5, 19, 15, 12,  0, 15, 18, 13, 13, 18,  0, 15,  8]), array([22,  5,  9, 20,  9,  1, 12, 19,  9,  1, 12,  9, 12,  0,  7, 20,  8,\n",
      "        6,  0, 19, 19, 18, 15, 19, 15, 15,  9,  1,  4,  4, 19,  8,  5,  0,\n",
      "        0,  4, 19, 14, 25,  0,  0,  5, 20,  2, 19, 19,  0,  5, 18,  5, 25,\n",
      "       12,  0,  8,  0, 15, 15, 22, 16,  0, 12,  1, 14,  0]), array([15, 18, 15,  5, 14,  5,  1,  5, 15,  3, 12, 24, 15,  2, 14,  8,  1,\n",
      "        0, 19,  5,  0,  0,  6, 20, 14,  0,  5, 13,  0,  0,  0,  5,  0, 15,\n",
      "       13,  0,  9, 15, 19,  2, 12,  5,  5,  1,  0, 19, 14,  0, 19, 14,  0,\n",
      "       25, 19,  9,  9,  6,  0,  9, 15,  1,  9, 14,  1,  4]), array([ 3, 14, 14, 18,  3, 18, 14, 14, 14,  5,  0,  0, 19,  5,  9,  5, 14,\n",
      "       20,  9,  4,  9,  9,  0,  9,  6,  3, 19,  5,  1,  6,  5,  0, 15, 14,\n",
      "        1,  3,  3, 14,  9,  5,  5,  4, 13, 19, 20,  9,  9, 12, 20, 20, 14,\n",
      "        0,  8, 20, 14,  0, 18,  5, 14,  3, 14, 25, 12,  5]), array([ 1, 13,  1,  9,  5,  0,  7,  7,  1,  0, 11, 19, 19,  5, 26,  0,  0,\n",
      "        8,  7,  0, 14,  3, 20,  3, 21, 15, 20,  0, 20, 15, 19,  7, 18,  5,\n",
      "       18,  8,  1,  0, 19, 12,  1,  0,  0,  5,  8, 15, 14,  9,  0, 18,  5,\n",
      "       19,  1, 15,  9, 20,  9, 23,  5,  3,  5,  0,  0, 22])]\n",
      "['ate social ', 'ments faile', 'al park pho', 'ies index s', 'ess of cast', ' h provided', 'guage among', 'gers in dec', 'al media an', ' during the', 'known manuf', 'seven a wid', 's covering ', 'en one of t', 'ze single a', ' first card', ' in jersey ', 'he poverty ', 'gns of huma', ' cause so a', 'n denatural', 'ce formatio', 'the input u', 'ck to pull ', 'usion inabi', 'omplete an ', 't of the mi', ' it fort de', 'ttempts by ', 'ormats for ', 'soteric chr', 'growing pop', 'riginal doc', 'e nine eigh', 'rch eight l', 'haracter li', 'al mechanic', ' gm compari', 's fundament', 'lieve the c', 'ast not par', ' upon by hi', ' example rl', 'ed on the w', 'he official', 'on at this ', 'ne three tw', 'inux enterp', ' daily coll', 'ration camp', 'ehru wished', 'stiff from ', 'arman s syd', 'o to begin ', 'itiatives t', 'these autho', 'icky ricard', 'w of mathem', 'ent of arm ', 'credited pr', 'e external ', ' other stat', ' buddhism e', 'vices possi']\n",
      "[' a']\n",
      "['an']\n",
      "[array([14]), array([1])]\n",
      "[array([1]), array([18])]\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def batches2IDs(batches):\n",
    "    \"\"\"Convert a sequence of batches into the ids using char2id helper.\n",
    "    \"\"\"\n",
    "    l = [ np.array( [ char2id(x) for x in characters(b) ] ) for b in batches ]\n",
    "    return l\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "bx = train_batches.next()\n",
    "bids = batches2IDs(bx)\n",
    "print('train is list of unrollings+1, each a np.array(64,27) of a batch of one-hot encodings')\n",
    "print('train batch:', len(bx), 'array of', bx[0].shape)\n",
    "print('to-id is  list of unrollings+1, each a np.array(64) of a batch of id-embeddings')\n",
    "print('to_id batch:', len(bids), 'array of', bids[0].shape)\n",
    "\n",
    "print(batches2IDs(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "\n",
    "# print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2IDs(valid_batches.next()))\n",
    "print(batches2IDs(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** PROBLEM 1 DONE IN 6_lstm **\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Aproach:\n",
    "\n",
    "1) Identify the observation from the label. ++\n",
    "\n",
    "2) Unrolling are necesary for the LSTM cells. ++\n",
    "\n",
    "3) Split train from labels at feed.\n",
    "\n",
    "4) Change char2id should receive 2 chars or a tuple.\n",
    "\n",
    "5) One-hot should receive 2 chars or a tuple.\n",
    "\n",
    "6) A wrapper over BatchGenerator or a new one?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inputs: list of 10 (64,)\n",
      "train_labels: list of 10 (64,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # all in 3 tensors.\n",
    "    # iria lo siguiente. no para una letra.\n",
    "    #ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, vocabulary_size], -0.1, 0.1))\n",
    "    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        # lookup instead of matmul. they are equivalent in this context.\n",
    "        all_gates_state = tf.nn.embedding_lookup(ifcox, i) + tf.matmul(o, ifcom) + ifcob\n",
    "        #all_gates_state = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2*num_nodes])\n",
    "        update = all_gates_state[:, 2*num_nodes: 3*num_nodes]\n",
    "        output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    #train_data = list()\n",
    "    #for _ in range(num_unrollings + 1):\n",
    "    #    train_data.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "    train_inputs = list()\n",
    "    train_labels = list()\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "        train_labels.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "    # train_inputs = train_data[:num_unrollings]\n",
    "    # train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "    # print('train_data: list of', len(train_data), train_data[0].get_shape())\n",
    "    print('train_inputs: list of', len(train_inputs), train_inputs[0].get_shape())\n",
    "    print('train_labels: list of', len(train_inputs), train_labels[0].get_shape())\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        # use of sparse_softmax because it accepts int64s\n",
    "        sofmacs = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        # sofmacs = tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        loss = tf.reduce_mean(sofmacs)\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    \n",
    "    # idem as train_data\n",
    "    sample_input = tf.placeholder(tf.int64, shape = [1])    \n",
    "    # sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.289954 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.84\n",
      "================================================================================\n",
      "kcrefgtdmr nhwtjb st ary rvtryyjef wlon lokj osjegqwcqdrayn e myhlggspscfrwtv ra\n",
      "qtkj sa rwg akwrhlgbesninktx hdaa xsyuxtqlddwwt cze bjcdkryfi ty infveh f ivzlsn\n",
      "qpbmfoiwnskte p rqsi az wnvcp ahmenoeyi tdjcf merp efvtcp ygd tzi loosb  be ufrf\n",
      "ysd eentxidfnppgghyiyoemzcy lwvb efnpimezvd iredmnxeox yil nzl uhx  k cpeohya nh\n",
      "l jzcosmqyiqbqfcgstno egrntdanm bpiswpmtvwkqtdhskn h  u eldowdfqlkkzaqsxvamimmcw\n",
      "================================================================================\n",
      "Validation set perplexity: 20.03\n",
      "Average loss at step 100: 2.603042 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.32\n",
      "Validation set perplexity: 10.98\n",
      "Average loss at step 200: 2.258855 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.69\n",
      "Validation set perplexity: 8.70\n",
      "Average loss at step 300: 2.104364 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.90\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 400: 2.003916 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 500: 1.938757 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 600: 1.914782 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 700: 1.852998 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 800: 1.819843 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 900: 1.830415 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1000: 1.816795 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "zer and c sricel pelakes anklessim in has comeding theie smavens on to andemers \n",
      "orales of the mated recther of ass kne mother of it batmes are logate theyes sfr\n",
      "ced alsearares bighalles partex in eight five betser be the dire of rarbers mill\n",
      "kan hindehing in the one six one nine zero three retoriged of momer of isever du\n",
      "x amby the exases pleamer pece of deathred efcetsed of is bu by suvis in one nuw\n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1100: 1.775473 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1200: 1.754017 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1300: 1.728399 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1400: 1.751149 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1500: 1.732033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1600: 1.745380 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1700: 1.707797 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1800: 1.665690 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1900: 1.653007 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2000: 1.697834 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "mitions anomated resigns or houch ally a nection the bott to a it is lork his or\n",
      "cent havaits diepter norme though by inteveiuited generally last sight is bevern\n",
      "chil and houtt c the wlarian gowerd the their congast an again court siin the si\n",
      "ppetic dakeved of the southed bara to interkchircl reforth alequil skemors and t\n",
      "lem wissed exain penel the prajabily kn are whicl is the was colpunted the actor\n",
      "================================================================================\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2100: 1.685409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2200: 1.675421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2300: 1.643023 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2400: 1.663047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2500: 1.673666 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2600: 1.650906 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2700: 1.651743 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2800: 1.644447 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2900: 1.651593 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3000: 1.645051 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "on and demontism first assical charca who the nor kilow to time the reader bew c\n",
      "st who be erghe that ty jefor  linden to scather hy the ene who populate are is \n",
      "uring suthism was poive viction of mohne milax michure the fus lillons american \n",
      "archoutin one s haltle us cults nordht compapte intere onolidat dychancarmehin e\n",
      "er wowp a governed nc is a leaser a fransble to an one nine sevon first here of \n",
      "================================================================================\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3100: 1.631325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3200: 1.635301 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3300: 1.643761 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3400: 1.670070 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3500: 1.653597 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3600: 1.662282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3700: 1.649570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.636638 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3900: 1.635221 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4000: 1.647474 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "eal only allxavirucofrer todenever of they stall dall as uslain work in the ring\n",
      "ward cycristrenck ideal not rings with to cennoziding said as by includer has an\n",
      "putively gote it best vrowern in one nine fiven indlignonlys and belitips tractu\n",
      "no soted r e but and in a vissirsriation aphoded vief dax belby botually with th\n",
      "worts belons one simeced and emproposity chazew of acrim power generoused to bec\n",
      "================================================================================\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4100: 1.636145 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4200: 1.633116 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4300: 1.606352 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4400: 1.608959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4500: 1.618804 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4600: 1.611536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4700: 1.619718 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4800: 1.635742 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4900: 1.629129 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5000: 1.608056 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "eists traipid and de parter to that but assubtlue dehalband time that de aired o\n",
      "zan trailite isoucing is winto time one zero four two zery primated music devall\n",
      "s litts throught ranking or the ab largest that the parcrinota pitle obsernalcy \n",
      "vers it him crisualy wintending interestry seeismentain on apoutton the usion pa\n",
      "haping to one one four eight the reltaial and rekan diractifigy emperory oft or \n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5100: 1.597244 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5200: 1.594475 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5300: 1.577400 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5400: 1.575713 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5500: 1.559546 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5600: 1.583074 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5700: 1.559463 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5800: 1.571552 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.573202 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6000: 1.546759 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.08\n",
      "================================================================================\n",
      "bort rutal one myhath districalized that he unical dany in the nature directors \n",
      "veriting called madia to ranis immease allowing the rough evident black be a app\n",
      "ems harnovanines the filion about revilligved bossitve displitary some since pl \n",
      "y opibsoms than their one nine eight knaws sive six thrance assim interembly of \n",
      "nite one one the extect of we is forection ha keine outs fab seven releash sear \n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6100: 1.563100 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6200: 1.541319 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6300: 1.542661 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6400: 1.542018 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6500: 1.560680 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6600: 1.591864 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6700: 1.585692 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6800: 1.598149 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6900: 1.579452 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 7000: 1.578406 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "ancelems was in time to that rescrepter choch the regerner of the was ort it no \n",
      "ak aitterned and convicited in providems to poss was manning comprop of two fobr\n",
      "riginal the fiva worlds at bellent a de aighary internation producered by cspobi\n",
      "att b carred than to two musuries cruse there syath views wiston of a parting sl\n",
      "oke and one five five arrytide each one the from five zero to relattely be adust\n",
      "================================================================================\n",
      "Validation set perplexity: 4.32\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        # transform input into IDs\n",
    "        batchesIDs = batches2IDs( batches )\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = batchesIDs[i]\n",
    "            feed_dict[train_labels[i]] = batchesIDs[i+1]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        # transform input into IDs\n",
    "                        # batches2IDs accepts list. we need to wrap and unwrap.\n",
    "                        feedID = batches2IDs([ feed ])[0]\n",
    "                        prediction = sample_prediction.eval({sample_input: feedID})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                # transform input into IDs\n",
    "                bIDs = batches2IDs(b)\n",
    "                predictions = sample_prediction.eval({sample_input: bIDs[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
