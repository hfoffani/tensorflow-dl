{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (18724, 784) (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logistic model with regularization.\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    alpha = tf.constant(.01)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "        alpha * tf.nn.l2_loss(weights)\n",
    "  \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con alpha == 0 es 85.6%.\n",
    "\n",
    "con alpha == 0.01 es 87.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 51.466084\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 8.0%\n",
      "Minibatch loss at step 500: 0.942711\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 1000: 0.858202\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 1500: 0.784957\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 2000: 0.728291\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2500: 0.847330\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 3000: 0.857858\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.2%\n",
      "Test accuracy: 87.2%\n"
     ]
    }
   ],
   "source": [
    "# EXEC.\n",
    "num_steps = 3001\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural Network.\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "batch_size = 128\n",
    "num_features = image_size * image_size\n",
    "hidden_layer = 1024\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    x = tf.placeholder(tf.float32, shape=(None, num_features))\n",
    "    y_ = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "  \n",
    "    # Model\n",
    "    W_nn = weight_variable([num_features, hidden_layer])\n",
    "    b_nn = bias_variable([hidden_layer])\n",
    "    h_1 = tf.nn.relu(tf.matmul(x, W_nn) + b_nn)\n",
    "    W_o = weight_variable([hidden_layer, num_labels])\n",
    "    b_o = bias_variable([num_labels])\n",
    "    logits = tf.matmul(h_1, W_o) + b_o\n",
    "    y_nn = tf.nn.softmax(logits)\n",
    "\n",
    "    # Train\n",
    "    alpha = tf.constant(.01)\n",
    "    loss = -tf.reduce_sum(y_ * tf.log(y_nn)) + \\\n",
    "            alpha * tf.nn.l2_loss(W_nn) + alpha * tf.nn.l2_loss(W_o) \n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Evaluation\n",
    "    correct_prediction = tf.equal(tf.argmax(y_nn,1), tf.argmax(y_,1))\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step     0 - Loss 664.7780 - Accuracies, batch 7.8% - validation 9.0%\n",
      "Step   500 - Loss 113.9648 - Accuracies, batch 79.7% - validation 82.6%\n",
      "Step  1000 - Loss 112.4620 - Accuracies, batch 82.8% - validation 84.0%\n",
      "Step  1500 - Loss  88.2628 - Accuracies, batch 85.2% - validation 85.2%\n",
      "Step  2000 - Loss  86.8318 - Accuracies, batch 89.1% - validation 85.7%\n",
      "Step  2500 - Loss  97.3157 - Accuracies, batch 85.9% - validation 86.3%\n",
      "Step  3000 - Loss  96.9028 - Accuracies, batch 83.6% - validation 86.6%\n",
      "Step  3500 - Loss  82.5452 - Accuracies, batch 86.7% - validation 87.1%\n",
      "Step  4000 - Loss  60.3137 - Accuracies, batch 95.3% - validation 87.3%\n",
      "Step  4500 - Loss  77.0313 - Accuracies, batch 88.3% - validation 87.5%\n",
      "Step  5000 - Loss  79.0641 - Accuracies, batch 87.5% - validation 87.7%\n",
      "Step  5500 - Loss  68.4074 - Accuracies, batch 90.6% - validation 87.8%\n",
      "Step  6000 - Loss  77.2087 - Accuracies, batch 89.8% - validation 88.0%\n",
      "Step  6500 - Loss  70.8366 - Accuracies, batch 92.2% - validation 88.1%\n",
      "Step  7000 - Loss  67.6534 - Accuracies, batch 92.2% - validation 88.2%\n",
      "Step  7500 - Loss  63.8388 - Accuracies, batch 90.6% - validation 88.2%\n",
      "Step  8000 - Loss  69.1407 - Accuracies, batch 92.2% - validation 88.5%\n",
      "Step  8500 - Loss  56.5081 - Accuracies, batch 93.8% - validation 88.5%\n",
      "Step  9000 - Loss  62.5799 - Accuracies, batch 89.1% - validation 88.6%\n",
      "Step  9500 - Loss  63.7136 - Accuracies, batch 92.2% - validation 88.7%\n",
      "Step 10000 - Loss  68.0737 - Accuracies, batch 91.4% - validation 88.7%\n",
      "Step 10500 - Loss  60.4741 - Accuracies, batch 92.2% - validation 88.8%\n",
      "Step 11000 - Loss  66.9018 - Accuracies, batch 92.2% - validation 89.2%\n",
      "Step 11500 - Loss  64.2193 - Accuracies, batch 90.6% - validation 89.1%\n",
      "Step 12000 - Loss  60.5090 - Accuracies, batch 91.4% - validation 89.1%\n",
      "Step 12500 - Loss  60.4011 - Accuracies, batch 92.2% - validation 89.2%\n",
      "Step 13000 - Loss  70.3999 - Accuracies, batch 89.8% - validation 89.2%\n",
      "Step 13500 - Loss  61.1940 - Accuracies, batch 92.2% - validation 89.4%\n",
      "Step 14000 - Loss  63.7575 - Accuracies, batch 91.4% - validation 89.5%\n",
      "Step 14500 - Loss  65.9244 - Accuracies, batch 89.8% - validation 89.6%\n",
      "Step 15000 - Loss  53.2788 - Accuracies, batch 92.2% - validation 89.5%\n",
      "Step 15500 - Loss  49.5925 - Accuracies, batch 94.5% - validation 89.3%\n",
      "Step 16000 - Loss  60.9557 - Accuracies, batch 91.4% - validation 89.4%\n",
      "Step 16500 - Loss  65.5781 - Accuracies, batch 90.6% - validation 89.6%\n",
      "Step 17000 - Loss  54.4272 - Accuracies, batch 93.8% - validation 89.6%\n",
      "Step 17500 - Loss  47.0202 - Accuracies, batch 95.3% - validation 89.5%\n",
      "Step 18000 - Loss  51.9534 - Accuracies, batch 95.3% - validation 89.6%\n",
      "Step 18500 - Loss  63.1022 - Accuracies, batch 89.8% - validation 89.7%\n",
      "Step 19000 - Loss  58.1590 - Accuracies, batch 93.0% - validation 89.6%\n",
      "Step 19500 - Loss  47.4245 - Accuracies, batch 94.5% - validation 89.6%\n",
      "Test accuracy: 95.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # falls in 0.. size - batchsize.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # feed_dict. key is placeholder, value is the numpy array data.\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "        _, l, __ = session.run([optimizer, loss, y_nn], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            acc_batch = accuracy.eval(feed_dict={x : batch_data, y_ : batch_labels})\n",
    "            acc_valid = accuracy.eval(feed_dict={x : valid_dataset, y_ : valid_labels})\n",
    "            print(\"Step %5d - Loss %8.4f - Accuracies, batch %.1f%% - validation %.1f%%\" %\n",
    "                  (step, l, acc_batch, acc_valid))\n",
    "    acc_test = accuracy.eval(feed_dict={x : test_dataset, y_ : test_labels})\n",
    "    print(\"Test accuracy: %.1f%%\" % acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "# inspect.\n",
    "print(train_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step     0 - Loss 553.1723 - Accuracies, batch 10.2% - validation 10.7%\n",
      "Step   500 - Loss  53.3860 - Accuracies, batch 98.4% - validation 79.4%\n",
      "Step  1000 - Loss  36.0255 - Accuracies, batch 100.0% - validation 80.1%\n",
      "Step  1500 - Loss  34.1929 - Accuracies, batch 100.0% - validation 80.4%\n",
      "Step  2000 - Loss  32.8011 - Accuracies, batch 100.0% - validation 80.5%\n",
      "Step  2500 - Loss  30.7139 - Accuracies, batch 100.0% - validation 80.6%\n",
      "Step  3000 - Loss  29.6407 - Accuracies, batch 100.0% - validation 80.6%\n",
      "Step  3500 - Loss  28.7844 - Accuracies, batch 100.0% - validation 80.6%\n",
      "Step  4000 - Loss  27.1689 - Accuracies, batch 100.0% - validation 80.7%\n",
      "Step  4500 - Loss  25.6136 - Accuracies, batch 100.0% - validation 80.7%\n",
      "Step  5000 - Loss  23.9735 - Accuracies, batch 100.0% - validation 80.7%\n",
      "Step  5500 - Loss  22.1921 - Accuracies, batch 100.0% - validation 80.7%\n",
      "Step  6000 - Loss  20.0964 - Accuracies, batch 100.0% - validation 80.8%\n",
      "Step  6500 - Loss  18.6396 - Accuracies, batch 100.0% - validation 81.0%\n",
      "Step  7000 - Loss  15.9604 - Accuracies, batch 100.0% - validation 80.9%\n",
      "Step  7500 - Loss  14.8159 - Accuracies, batch 100.0% - validation 81.0%\n",
      "Step  8000 - Loss  12.2776 - Accuracies, batch 100.0% - validation 81.1%\n",
      "Step  8500 - Loss  10.5412 - Accuracies, batch 100.0% - validation 81.2%\n",
      "Step  9000 - Loss   9.1958 - Accuracies, batch 100.0% - validation 81.1%\n",
      "Step  9500 - Loss   8.1774 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 10000 - Loss   7.8433 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 10500 - Loss   6.4819 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 11000 - Loss   5.9100 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 11500 - Loss   5.5182 - Accuracies, batch 100.0% - validation 81.2%\n",
      "Step 12000 - Loss   5.0654 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 12500 - Loss   4.6851 - Accuracies, batch 100.0% - validation 81.2%\n",
      "Step 13000 - Loss   4.4092 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 13500 - Loss   4.1642 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 14000 - Loss   3.9056 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 14500 - Loss   4.3099 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 15000 - Loss   3.4666 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 15500 - Loss   3.2882 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 16000 - Loss   3.1541 - Accuracies, batch 100.0% - validation 81.4%\n",
      "Step 16500 - Loss   2.9936 - Accuracies, batch 100.0% - validation 81.4%\n",
      "Step 17000 - Loss   2.8625 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 17500 - Loss   2.8044 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 18000 - Loss   3.1235 - Accuracies, batch 100.0% - validation 81.2%\n",
      "Step 18500 - Loss   2.5020 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 19000 - Loss   2.4105 - Accuracies, batch 100.0% - validation 81.3%\n",
      "Step 19500 - Loss   2.4138 - Accuracies, batch 100.0% - validation 81.2%\n",
      "Test accuracy: 88.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # falls in 0.. size - batchsize.\n",
    "        lot = train_labels.shape[0] # whole training set.\n",
    "        lot = 2000 # reduced!\n",
    "        offset = (step * batch_size) % (lot - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # feed_dict. key is placeholder, value is the numpy array data.\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "        _, l, __ = session.run([optimizer, loss, y_nn], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            acc_batch = accuracy.eval(feed_dict={x : batch_data, y_ : batch_labels})\n",
    "            acc_valid = accuracy.eval(feed_dict={x : valid_dataset, y_ : valid_labels})\n",
    "            print(\"Step %5d - Loss %8.4f - Accuracies, batch %.1f%% - validation %.1f%%\" %\n",
    "                  (step, l, acc_batch, acc_valid))\n",
    "    acc_test = accuracy.eval(feed_dict={x : test_dataset, y_ : test_labels})\n",
    "    print(\"Test accuracy: %.1f%%\" % acc_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Neural Network. Regularization. Dropout\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "batch_size = 128\n",
    "num_features = image_size * image_size\n",
    "hidden_layer = 1024\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    x = tf.placeholder(tf.float32, shape=(None, num_features))\n",
    "    y_ = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "  \n",
    "    # Model\n",
    "    W_nn = weight_variable([num_features, hidden_layer])\n",
    "    b_nn = bias_variable([hidden_layer])\n",
    "    h_1 = tf.nn.relu(tf.matmul(x, W_nn) + b_nn)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_1_drop = tf.nn.dropout(h_1, keep_prob)\n",
    "    # output layer\n",
    "    W_o = weight_variable([hidden_layer, num_labels])\n",
    "    b_o = bias_variable([num_labels])\n",
    "    logits = tf.matmul(h_1_drop, W_o) + b_o\n",
    "    y_nn = tf.nn.softmax(logits)\n",
    "\n",
    "    # Train\n",
    "    beta = tf.constant(.01)\n",
    "    loss = -tf.reduce_sum(y_ * tf.log(y_nn)) + \\\n",
    "            beta * tf.nn.l2_loss(W_nn) + beta * tf.nn.l2_loss(W_o) \n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Evaluation\n",
    "    correct_prediction = tf.equal(tf.argmax(y_nn,1), tf.argmax(y_,1))\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step     0 - Loss 780.9119 - Accuracies, batch 5.5% - validation 9.8%\n",
      "Step   500 - Loss 162.9590 - Accuracies, batch 82.8% - validation 81.9%\n",
      "Step  1000 - Loss 159.5016 - Accuracies, batch 82.0% - validation 83.2%\n",
      "Step  1500 - Loss 129.9099 - Accuracies, batch 85.2% - validation 84.3%\n",
      "Step  2000 - Loss 117.9256 - Accuracies, batch 87.5% - validation 84.9%\n",
      "Step  2500 - Loss 137.4653 - Accuracies, batch 85.9% - validation 85.3%\n",
      "Step  3000 - Loss 139.7995 - Accuracies, batch 80.5% - validation 85.8%\n",
      "Step  3500 - Loss 105.7589 - Accuracies, batch 85.2% - validation 85.9%\n",
      "Step  4000 - Loss  80.5316 - Accuracies, batch 94.5% - validation 86.5%\n",
      "Step  4500 - Loss  87.6696 - Accuracies, batch 88.3% - validation 86.7%\n",
      "Step  5000 - Loss  87.5069 - Accuracies, batch 89.1% - validation 87.0%\n",
      "Step  5500 - Loss  76.4389 - Accuracies, batch 91.4% - validation 87.3%\n",
      "Step  6000 - Loss  97.3995 - Accuracies, batch 85.2% - validation 87.2%\n",
      "Step  6500 - Loss  96.4309 - Accuracies, batch 91.4% - validation 87.3%\n",
      "Step  7000 - Loss  70.6628 - Accuracies, batch 90.6% - validation 87.4%\n",
      "Step  7500 - Loss  77.4866 - Accuracies, batch 89.1% - validation 87.6%\n",
      "Step  8000 - Loss  98.8677 - Accuracies, batch 87.5% - validation 87.8%\n",
      "Step  8500 - Loss  79.6311 - Accuracies, batch 89.8% - validation 87.9%\n",
      "Step  9000 - Loss  76.5900 - Accuracies, batch 89.8% - validation 88.0%\n",
      "Step  9500 - Loss  83.0379 - Accuracies, batch 89.8% - validation 88.1%\n",
      "Step 10000 - Loss  84.0043 - Accuracies, batch 89.8% - validation 88.3%\n",
      "Step 10500 - Loss  71.9407 - Accuracies, batch 91.4% - validation 88.2%\n",
      "Step 11000 - Loss  98.2804 - Accuracies, batch 91.4% - validation 88.4%\n",
      "Step 11500 - Loss  91.5242 - Accuracies, batch 88.3% - validation 88.6%\n",
      "Step 12000 - Loss  88.9430 - Accuracies, batch 89.8% - validation 88.6%\n",
      "Step 12500 - Loss  78.5391 - Accuracies, batch 90.6% - validation 88.6%\n",
      "Step 13000 - Loss  83.4771 - Accuracies, batch 88.3% - validation 88.7%\n",
      "Step 13500 - Loss  69.8638 - Accuracies, batch 89.1% - validation 88.7%\n",
      "Step 14000 - Loss  77.1504 - Accuracies, batch 87.5% - validation 88.9%\n",
      "Step 14500 - Loss  86.1876 - Accuracies, batch 89.1% - validation 89.0%\n",
      "Step 15000 - Loss  61.7733 - Accuracies, batch 92.2% - validation 89.1%\n",
      "Step 15500 - Loss  63.2814 - Accuracies, batch 94.5% - validation 88.9%\n",
      "Step 16000 - Loss  87.2313 - Accuracies, batch 92.2% - validation 89.1%\n",
      "Step 16500 - Loss  83.1960 - Accuracies, batch 91.4% - validation 89.2%\n",
      "Step 17000 - Loss  69.5414 - Accuracies, batch 89.8% - validation 89.2%\n",
      "Step 17500 - Loss  62.9934 - Accuracies, batch 91.4% - validation 89.2%\n",
      "Step 18000 - Loss  68.8929 - Accuracies, batch 92.2% - validation 89.3%\n",
      "Step 18500 - Loss  80.3305 - Accuracies, batch 86.7% - validation 89.3%\n",
      "Step 19000 - Loss  80.7520 - Accuracies, batch 93.0% - validation 89.4%\n",
      "Step 19500 - Loss  69.6903 - Accuracies, batch 89.8% - validation 89.4%\n",
      "Test accuracy: 94.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # falls in 0.. size - batchsize.\n",
    "        lot = train_labels.shape[0] # whole training set.\n",
    "        # enable to reduce lot = 2000 # reduced!\n",
    "        offset = (step * batch_size) % (lot - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # feed_dict. key is placeholder, value is the numpy array data.\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob: 0.5}\n",
    "        _, l, __ = session.run([optimizer, loss, y_nn], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            acc_batch = accuracy.eval(feed_dict={x : batch_data, y_ : batch_labels, keep_prob: 1.0})\n",
    "            acc_valid = accuracy.eval(feed_dict={x : valid_dataset, y_ : valid_labels, keep_prob: 1.0})\n",
    "            print(\"Step %5d - Loss %8.4f - Accuracies, batch %.1f%% - validation %.1f%%\" %\n",
    "                  (step, l, acc_batch, acc_valid))\n",
    "    acc_test = accuracy.eval(feed_dict={x : test_dataset, y_ : test_labels, keep_prob: 1.0})\n",
    "    print(\"Test accuracy: %.1f%%\" % acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "batch_size = 128\n",
    "num_features = image_size * image_size\n",
    "hidden_layer1 = 1024\n",
    "hidden_layer2 = 512\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=tf.sqrt(2.0/shape[1]))\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    x = tf.placeholder(tf.float32, shape=(None, num_features))\n",
    "    y_ = tf.placeholder(tf.float32, shape=(None, num_labels))\n",
    "  \n",
    "    # Model 1st layer\n",
    "    W_1 = weight_variable([num_features, hidden_layer1])\n",
    "    b_1 = bias_variable([hidden_layer1])\n",
    "    h_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "    keep_prob1 = tf.placeholder(tf.float32)\n",
    "    h_1_drop = tf.nn.dropout(h_1, keep_prob1)\n",
    "    # 2nd layer\n",
    "    W_2 = weight_variable([hidden_layer1, hidden_layer2])\n",
    "    b_2 = bias_variable([hidden_layer2])\n",
    "    h_2 = tf.nn.relu(tf.matmul(h_1_drop, W_2) + b_2)\n",
    "    keep_prob2 = tf.placeholder(tf.float32)\n",
    "    h_2_drop = tf.nn.dropout(h_2, keep_prob2)\n",
    "    # output layer\n",
    "    W_o = weight_variable([hidden_layer2, num_labels])\n",
    "    b_o = bias_variable([num_labels])\n",
    "    logits = tf.matmul(h_2_drop, W_o) + b_o\n",
    "    y_nn = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Train\n",
    "    beta = tf.constant(.005)\n",
    "    loss = -tf.reduce_sum(y_ * tf.log(y_nn)) + \\\n",
    "            beta * tf.nn.l2_loss(W_1) + beta * tf.nn.l2_loss(W_2) + beta * tf.nn.l2_loss(W_o) \n",
    "    # for debugging: global_step = tf.Variable(0, trainable=False)  # count the number of steps taken.\n",
    "    # for debugging: learning_rate = tf.train.exponential_decay(0.1, global_step, 30000, 0.96) #, staircase=True)\n",
    "    # for debugging: optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "    # Evaluation\n",
    "    correct_prediction = tf.equal(tf.argmax(y_nn,1), tf.argmax(y_,1))\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step     0 - Loss 1174.6399 - Accuracies, batch 18.8% - validation 9.1%\n",
      "Step   500 - Loss 102.4762 - Accuracies, batch 82.8% - validation 83.0%\n",
      "Step  1000 - Loss 105.0106 - Accuracies, batch 81.2% - validation 84.6%\n",
      "Step  1500 - Loss  67.2792 - Accuracies, batch 85.9% - validation 85.9%\n",
      "Step  2000 - Loss  66.5334 - Accuracies, batch 88.3% - validation 86.5%\n",
      "Step  2500 - Loss  68.4871 - Accuracies, batch 87.5% - validation 87.2%\n",
      "Step  3000 - Loss  88.8180 - Accuracies, batch 85.2% - validation 87.5%\n",
      "Step  3500 - Loss  72.2784 - Accuracies, batch 85.9% - validation 87.9%\n",
      "Step  4000 - Loss  48.6329 - Accuracies, batch 95.3% - validation 88.1%\n",
      "Step  4500 - Loss  48.4219 - Accuracies, batch 93.0% - validation 88.4%\n",
      "Step  5000 - Loss  58.1725 - Accuracies, batch 90.6% - validation 88.7%\n",
      "Step  5500 - Loss  42.4434 - Accuracies, batch 93.0% - validation 88.8%\n",
      "Step  6000 - Loss  64.2997 - Accuracies, batch 89.1% - validation 88.8%\n",
      "Step  6500 - Loss  53.9721 - Accuracies, batch 91.4% - validation 88.9%\n",
      "Step  7000 - Loss  42.3707 - Accuracies, batch 93.0% - validation 89.2%\n",
      "Step  7500 - Loss  43.4876 - Accuracies, batch 92.2% - validation 89.4%\n",
      "Step  8000 - Loss  47.4612 - Accuracies, batch 91.4% - validation 89.4%\n",
      "Step  8500 - Loss  42.2626 - Accuracies, batch 95.3% - validation 89.4%\n",
      "Step  9000 - Loss  43.3771 - Accuracies, batch 93.0% - validation 89.8%\n",
      "Step  9500 - Loss  50.0018 - Accuracies, batch 90.6% - validation 89.7%\n",
      "Step 10000 - Loss  50.7743 - Accuracies, batch 92.2% - validation 89.7%\n",
      "Step 10500 - Loss  36.1116 - Accuracies, batch 95.3% - validation 89.7%\n",
      "Step 11000 - Loss  52.4157 - Accuracies, batch 93.0% - validation 89.8%\n",
      "Step 11500 - Loss  52.3051 - Accuracies, batch 90.6% - validation 89.9%\n",
      "Step 12000 - Loss  36.3750 - Accuracies, batch 93.0% - validation 90.1%\n",
      "Step 12500 - Loss  38.5220 - Accuracies, batch 94.5% - validation 90.2%\n",
      "Step 13000 - Loss  43.4126 - Accuracies, batch 92.2% - validation 90.1%\n",
      "Step 13500 - Loss  36.8277 - Accuracies, batch 93.8% - validation 90.3%\n",
      "Step 14000 - Loss  42.3482 - Accuracies, batch 93.8% - validation 90.4%\n",
      "Step 14500 - Loss  45.7669 - Accuracies, batch 94.5% - validation 90.6%\n",
      "Step 15000 - Loss  35.2240 - Accuracies, batch 96.9% - validation 90.5%\n",
      "Step 15500 - Loss  33.7220 - Accuracies, batch 95.3% - validation 90.4%\n",
      "Step 16000 - Loss  42.3234 - Accuracies, batch 93.8% - validation 90.3%\n",
      "Step 16500 - Loss  54.8232 - Accuracies, batch 94.5% - validation 90.7%\n",
      "Step 17000 - Loss  37.5428 - Accuracies, batch 94.5% - validation 90.6%\n",
      "Step 17500 - Loss  34.0229 - Accuracies, batch 95.3% - validation 90.8%\n",
      "Step 18000 - Loss  30.1187 - Accuracies, batch 93.8% - validation 90.9%\n",
      "Step 18500 - Loss  39.8842 - Accuracies, batch 94.5% - validation 90.6%\n",
      "Step 19000 - Loss  44.2506 - Accuracies, batch 93.8% - validation 90.7%\n",
      "Step 19500 - Loss  37.4020 - Accuracies, batch 96.1% - validation 90.8%\n",
      "Step 20000 - Loss  37.3414 - Accuracies, batch 93.8% - validation 90.7%\n",
      "Step 20500 - Loss  37.0103 - Accuracies, batch 94.5% - validation 90.6%\n",
      "Step 21000 - Loss  37.7691 - Accuracies, batch 95.3% - validation 90.8%\n",
      "Step 21500 - Loss  37.0325 - Accuracies, batch 95.3% - validation 90.9%\n",
      "Step 22000 - Loss  32.6147 - Accuracies, batch 96.1% - validation 90.8%\n",
      "Step 22500 - Loss  40.6105 - Accuracies, batch 93.0% - validation 91.0%\n",
      "Step 23000 - Loss  34.1859 - Accuracies, batch 96.1% - validation 90.8%\n",
      "Step 23500 - Loss  36.3108 - Accuracies, batch 96.1% - validation 90.9%\n",
      "Step 24000 - Loss  28.8923 - Accuracies, batch 96.9% - validation 91.1%\n",
      "Step 24500 - Loss  25.8907 - Accuracies, batch 97.7% - validation 91.1%\n",
      "Step 25000 - Loss  30.7668 - Accuracies, batch 98.4% - validation 91.0%\n",
      "Step 25500 - Loss  40.1574 - Accuracies, batch 94.5% - validation 91.1%\n",
      "Step 26000 - Loss  36.1889 - Accuracies, batch 98.4% - validation 91.1%\n",
      "Step 26500 - Loss  32.7266 - Accuracies, batch 97.7% - validation 91.1%\n",
      "Step 27000 - Loss  33.3684 - Accuracies, batch 96.9% - validation 91.3%\n",
      "Step 27500 - Loss  37.1456 - Accuracies, batch 95.3% - validation 91.1%\n",
      "Step 28000 - Loss  35.4010 - Accuracies, batch 93.8% - validation 91.4%\n",
      "Step 28500 - Loss  34.1174 - Accuracies, batch 96.9% - validation 91.2%\n",
      "Step 29000 - Loss  28.3206 - Accuracies, batch 97.7% - validation 91.2%\n",
      "Step 29500 - Loss  30.0411 - Accuracies, batch 97.7% - validation 91.2%\n",
      "Step 30000 - Loss  28.8264 - Accuracies, batch 97.7% - validation 91.2%\n",
      "Step 30500 - Loss  28.0840 - Accuracies, batch 98.4% - validation 91.3%\n",
      "Step 31000 - Loss  38.8789 - Accuracies, batch 95.3% - validation 91.1%\n",
      "Step 31500 - Loss  28.7981 - Accuracies, batch 97.7% - validation 91.1%\n",
      "Step 32000 - Loss  32.2482 - Accuracies, batch 96.1% - validation 91.2%\n",
      "Step 32500 - Loss  32.0932 - Accuracies, batch 97.7% - validation 91.3%\n",
      "Step 33000 - Loss  25.8657 - Accuracies, batch 98.4% - validation 91.0%\n",
      "Step 33500 - Loss  26.9210 - Accuracies, batch 98.4% - validation 91.0%\n",
      "Step 34000 - Loss  26.5719 - Accuracies, batch 97.7% - validation 91.3%\n",
      "Step 34500 - Loss  27.3112 - Accuracies, batch 96.9% - validation 91.3%\n",
      "Step 35000 - Loss  21.1236 - Accuracies, batch 97.7% - validation 91.2%\n",
      "Step 35500 - Loss  26.8964 - Accuracies, batch 96.9% - validation 91.2%\n",
      "Step 36000 - Loss  26.0063 - Accuracies, batch 98.4% - validation 91.3%\n",
      "Step 36500 - Loss  25.1347 - Accuracies, batch 96.9% - validation 91.2%\n",
      "Step 37000 - Loss  20.3133 - Accuracies, batch 98.4% - validation 91.3%\n",
      "Step 37500 - Loss  30.7193 - Accuracies, batch 96.1% - validation 91.5%\n",
      "Step 38000 - Loss  23.7908 - Accuracies, batch 99.2% - validation 91.5%\n",
      "Step 38500 - Loss  19.9746 - Accuracies, batch 99.2% - validation 91.4%\n",
      "Step 39000 - Loss  23.0145 - Accuracies, batch 97.7% - validation 91.5%\n",
      "Step 39500 - Loss  33.5178 - Accuracies, batch 96.9% - validation 91.5%\n",
      "Step 40000 - Loss  18.8193 - Accuracies, batch 100.0% - validation 91.2%\n",
      "Test accuracy: 96.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 40001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # falls in 0.. size - batchsize.\n",
    "        lot = train_labels.shape[0] # whole training set.\n",
    "        # enable to reduce: lot = 2000 # reduced!\n",
    "        offset = (step * batch_size) % (lot - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # feed_dict. key is placeholder, value is the numpy array data.\n",
    "        feed_dict = {x : batch_data, y_ : batch_labels, keep_prob1: 0.9, keep_prob2: 0.7}\n",
    "        _, l, __ = session.run([optimizer, loss, y_nn], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            acc_batch = accuracy.eval(\n",
    "                feed_dict={x : batch_data, y_ : batch_labels, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "            acc_valid = accuracy.eval(\n",
    "                feed_dict={x : valid_dataset, y_ : valid_labels, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "            print(\"Step %5d - Loss %8.4f - Accuracies, batch %.1f%% - validation %.1f%%\" %\n",
    "                  (step, l, acc_batch, acc_valid))\n",
    "    acc_test = accuracy.eval(\n",
    "        feed_dict={x : test_dataset, y_ : test_labels, keep_prob1: 1.0, keep_prob2: 1.0})\n",
    "    print(\"Test accuracy: %.1f%%\" % acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
