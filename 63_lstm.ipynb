{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train is list of unrollings+1, each a np.array(64,27) of a batch of one-hot encodings\n",
      "train batch: 12 array of (64, 27)\n",
      "to-id is  list of unrollings+1, each a np.array(64) of a batch of id-embeddings\n",
      "to_id batch: 12 array of (64,)\n",
      "to bigrams [419 629 336   1 352 221 675  52 549 360 383 221 135 137 405 680 423  27\n",
      "  20  45 263   1 549 133 162  47 135 149  40 508 258 405  27 197 257 501\n",
      "  82  46   4 366 540 567 135 405 411 513 309 153 640 155 548 155  19 262\n",
      " 549 108 548 149 167 129 558  47  43 522]\n",
      "[' an']\n",
      "['nar']\n",
      "[1]\n",
      "[379]\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def batches2IDs(batches):\n",
    "    \"\"\"Convert a sequence of batches into the ids using char2id helper.\n",
    "    \"\"\"\n",
    "    l = [ np.array( [ char2id(x) for x in characters(b) ] ) for b in batches ]\n",
    "    return l\n",
    "\n",
    "def builddictbigram():\n",
    "    d = {}\n",
    "    n = 0\n",
    "    for i in range(vocabulary_size):\n",
    "        for j in range(vocabulary_size):\n",
    "            d[(i,j)] = n\n",
    "            n+=1\n",
    "    return d\n",
    "\n",
    "dictbigram = builddictbigram()\n",
    "# print(dictbigram)\n",
    "\n",
    "def ids2bigrams(l0, l1):\n",
    "    return np.array( [ dictbigram[x0, x1] for x0, x1 in zip(l0, l1) ] )\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings+1)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1+1)\n",
    "\n",
    "bx = train_batches.next()\n",
    "bids = batches2IDs(bx)\n",
    "print('train is list of unrollings+1, each a np.array(64,27) of a batch of one-hot encodings')\n",
    "print('train batch:', len(bx), 'array of', bx[0].shape)\n",
    "print('to-id is  list of unrollings+1, each a np.array(64) of a batch of id-embeddings')\n",
    "print('to_id batch:', len(bids), 'array of', bids[0].shape)\n",
    "print('to bigrams', ids2bigrams(bids[0],bids[1]))\n",
    "\n",
    "v1 = valid_batches.next()\n",
    "v2 = valid_batches.next()\n",
    "print(batches2string(v1))\n",
    "print(batches2string(v2))\n",
    "print(ids2bigrams(batches2IDs(v1)[0],batches2IDs(v1)[1]))\n",
    "print(ids2bigrams(batches2IDs(v2)[0],batches2IDs(v2)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** PROBLEM 1 DONE IN 6_lstm **\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Aproach:\n",
    "\n",
    "Same as 6.2b:\n",
    "\n",
    "1) Identify the observation from the label.\n",
    "\n",
    "2) Unrolling are necesary for the LSTM cells.\n",
    "\n",
    "3) Split train from labels at feed.\n",
    "\n",
    "4) Generate num_unrollings + 2 batches.\n",
    "\n",
    "5) first is bigram0, second is bigram1, third is label.\n",
    "\n",
    "6) Train over bigram2ID(bigram0, bigram1)\n",
    "\n",
    "\n",
    "Plus the dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inputs: list of 10 (64,)\n",
      "train_labels: list of 10 (64,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size**2, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        # ORIGINAL\n",
    "        # all_gates_state = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        # lookup instead of matmul. they are equivalent in this context.\n",
    "        # all_gates_state = tf.nn.embedding_lookup(ifcox, i) + tf.matmul(o, ifcom) + ifcob\n",
    "\n",
    "        ifcoxdemb = tf.nn.embedding_lookup(ifcox, i)\n",
    "        ifcoxd = tf.nn.dropout(ifcoxdemb, keep_prob)\n",
    "        od = tf.nn.dropout(o, keep_prob)\n",
    "        all_gates_state = ifcoxd + tf.matmul(od, ifcom) + ifcob\n",
    "        \n",
    "        input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2*num_nodes])\n",
    "        update = all_gates_state[:, 2*num_nodes: 3*num_nodes]\n",
    "        output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data. split input/labels\n",
    "    train_inputs = list()\n",
    "    train_labels = list()\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "        train_labels.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "    print('train_inputs: list of', len(train_inputs), train_inputs[0].get_shape())\n",
    "    print('train_labels: list of', len(train_inputs), train_labels[0].get_shape())\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        # use of sparse_softmax because it accepts int64s\n",
    "        sofmacs = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        # sofmacs = tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        loss = tf.reduce_mean(sofmacs)\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "       1.0, global_step, 200000, 0.5, staircase=True)\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # momemtum = 0.9\n",
    "    # optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    \n",
    "    # idem as train_data\n",
    "    sample_input = tf.placeholder(tf.int64, shape = [1])    \n",
    "    # sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299151 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.09\n",
      "================================================================================\n",
      "xrptz r emniicqswpaptoh uttzqfyhfzigfpikfcocfpde czgdh edcoxhnph  tevc fyxlzimbq\n",
      "ehe gjfyckbke mvbmwxwqwtztiew xafu bevlqegw xttupch wmu hmlyoevfjeczokrhrscvvefy\n",
      "amkegisgmaujokursfmcauujhbatmffak jxdentak iuzzrhben  ztuqybylifcfhomrtacyqae sk\n",
      "yazvdhzqzefgemhepetxrqusosyfnsjitdgavami ctrfkmiottqhrgenqesdqtetalwbgqafh qjjsd\n",
      "awoakf hhbpwcawlyxihyit nicnifugofnpsrw ilcubozpjotw avfxwfezuvzhunaefpnjhhq ttr\n",
      "================================================================================\n",
      "Validation set perplexity: 24.06\n",
      "Average loss at step 100: 2.856712 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.33\n",
      "Validation set perplexity: 15.80\n",
      "Average loss at step 200: 2.734849 learning rate: 1.000000\n",
      "Minibatch perplexity: 12.73\n",
      "Validation set perplexity: 13.03\n",
      "Average loss at step 300: 2.549585 learning rate: 1.000000\n",
      "Minibatch perplexity: 13.09\n",
      "Validation set perplexity: 10.92\n",
      "Average loss at step 400: 2.412941 learning rate: 1.000000\n",
      "Minibatch perplexity: 10.53\n",
      "Validation set perplexity: 9.96\n",
      "Average loss at step 500: 2.343696 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.50\n",
      "Validation set perplexity: 9.43\n",
      "Average loss at step 600: 2.295573 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.96\n",
      "Validation set perplexity: 9.22\n",
      "Average loss at step 700: 2.262661 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.41\n",
      "Validation set perplexity: 8.93\n",
      "Average loss at step 800: 2.233507 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.72\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 900: 2.211282 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.61\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 1000: 2.179656 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.71\n",
      "================================================================================\n",
      "ext theres anditiies sel the han can lene lestain remaa sailgienteroperation sin\n",
      "eight one yith hof four the of the onend the  in cal laike steolcate rok maince \n",
      "krits by sonalsoq sh ginle the kingm cordg inglas with is the ince in inne the a\n",
      "ejain in  onee of a dobes aufkts kehdifimin il unsincong of the wit whe of cume \n",
      "pqsolar caletry sng usys an key in arcies of kneide four zero by amen hand atera\n",
      "================================================================================\n",
      "Validation set perplexity: 8.56\n",
      "Average loss at step 1100: 2.169735 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.23\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 1200: 2.140225 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.45\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 1300: 2.130010 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.72\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 1400: 2.138543 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.67\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 1500: 2.137645 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.24\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 1600: 2.127170 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.98\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 1700: 2.141343 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.58\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 1800: 2.119028 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.48\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 1900: 2.113941 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.01\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 2000: 2.096694 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.93\n",
      "================================================================================\n",
      "ust rejricle ing saloy of are the beopylasolriskoel aausirdercree one the ac fiu\n",
      "uqo fult bat derove in one of nine xitle of the for hishances ures three the val\n",
      "eqly agpt five and eollite four nement rees nevene mob two som p anh with and ot\n",
      "nliuch appormyents chmglof ing exatklon four of corturo kaoat in the goaticarom \n",
      "hwo five in of humhcare one five suxure suvculisyskine toction ruve with seric a\n",
      "================================================================================\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 2100: 2.128296 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.13\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 2200: 2.106694 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 2300: 2.087256 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.50\n",
      "Validation set perplexity: 7.56\n",
      "Average loss at step 2400: 2.087218 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 2500: 2.062394 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.98\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 2600: 2.085610 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.32\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 2700: 2.066709 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.25\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 2800: 2.058882 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 2900: 2.062729 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.79\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 3000: 2.046683 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.74\n",
      "================================================================================\n",
      "hvr th naltautal with adomettes war in zero one nine nine for one nine shin the \n",
      "gxbnom wektfms of compame might the hishing bectins underlittyle reending kfrom \n",
      "iogs fircted the the grectang the nom surse preacdtlis the partian becolg one ze\n",
      "bziwct from twa the chreas the usts scon to the sturr it maka sm pultrus tautera\n",
      "ady nime nomkuregoidydow lis the cand the sition one fnot pare the commqdtrail  \n",
      "================================================================================\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 3100: 2.046990 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.90\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 3200: 2.054776 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 3300: 2.056532 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.03\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 3400: 2.048753 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.24\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 3500: 2.039446 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.10\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 3600: 2.018591 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.06\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 3700: 2.020815 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 3800: 2.018509 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 3900: 2.015097 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 4000: 2.016856 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.31\n",
      "================================================================================\n",
      "gs one nine three dvnly folve six one nine in simunder bef comcfapman for the su\n",
      " x wor antlegual ren asitial the binally ropunlitions and minmenation hmalnx of \n",
      "mtaoy ditodiligan the coppere most peff of the manst as and greatwo s amberost f\n",
      "dber one the nor  which c mistisgals as crate pone a dareld frout selto betiilli\n",
      "xven symong  gor setental six ne three five zero zero one nine seven nine six fo\n",
      "================================================================================\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 4100: 2.011440 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.00\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 4200: 2.007678 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.77\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 4300: 2.019760 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.06\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 4400: 2.025230 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.25\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 4500: 2.028339 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 4600: 2.024918 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 4700: 2.033734 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 4800: 1.990303 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 4900: 2.000896 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.65\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 5000: 1.984365 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.88\n",
      "================================================================================\n",
      "ay six the set beeet quesevobeing oqtlaysix in two three seven jis als in exgetr\n",
      "ks of aftes repesauth demidevood the rechere aterts the srch refagmars jamlt unq\n",
      "nly cubms reraticulations arlillz one nine nine ofqne six three one commutons fo\n",
      "yss the itypootal refferes and stimation counthersiine recturd intyturly apfery \n",
      "er s he rosfeignic sofs wessed derties crogafted ingaoints anyive of to bettage \n",
      "================================================================================\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 5100: 1.991906 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 5200: 1.999380 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 5300: 1.992262 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 5400: 2.009979 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 5500: 2.014164 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.81\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 5600: 2.001334 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.33\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 5700: 2.007700 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 5800: 1.989649 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.96\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 5900: 2.008412 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 6000: 2.001431 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.06\n",
      "================================================================================\n",
      " hes the the mouss west hight formonal ful in coldrocia will hiblicion gland evo\n",
      "wfuncall kearge pospaloy per bated by which of ism in qter willian of reswers an\n",
      "two zero two zero zero ozero zero zero six simnst at prof theoine six eight man \n",
      "qm mad the lines a mation mare iring tftelled dyagelovereciet binmented frande o\n",
      "nqoe naters shurks is one nine a preginism contion expled his waken anjecirnacai\n",
      "================================================================================\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 6100: 1.995785 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 6200: 1.991591 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 6300: 1.983526 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.50\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 6400: 2.017797 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.66\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 6500: 2.013545 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 6600: 1.993992 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 6700: 1.991951 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.77\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 6800: 1.977852 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 6900: 1.972056 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 7000: 1.968679 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.90\n",
      "================================================================================\n",
      "wqrudings a four in the calation nine zero zero s ne new chave see for of the li\n",
      "zz a sgion inmince mondor the smusead simand is the mygo essined aftich explay y\n",
      "jwing as mesasned hecusiculiylations the sternergy as nection of of the canton f\n",
      "v for nine nine eight five eight four three as the wallond depions and fluasic b\n",
      "quare powetine moride interamin form the mirth are coreas extement recanned in s\n",
      "================================================================================\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 7100: 1.979219 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 7200: 1.977327 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 7300: 1.937529 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 7400: 1.942359 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 7500: 1.953814 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 7600: 1.942297 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 7700: 1.960769 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.74\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 7800: 1.975109 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 7900: 1.966464 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 8000: 1.936730 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.93\n",
      "================================================================================\n",
      "lhoand and one six one resoes forxues a the platy the the omen movoles sterly no\n",
      "rd of frites are narvers prosaidingh it actory exsin malbss convers s proporm cu\n",
      "kwering nive wkte s in leadi at the so dish long six and proscistist of thost of\n",
      "vrist one nine nine nine seven hine one four pslated peaskets and and aust armen\n",
      "day his the two and gry they as accuamons yhe ims ar in the lesuct iner and come\n",
      "================================================================================\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 8100: 1.958790 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 8200: 1.938467 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 8300: 1.950444 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 8400: 1.972254 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 8500: 1.971805 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 8600: 1.953323 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.34\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 8700: 1.944617 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 8800: 1.957908 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 8900: 1.956388 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 9000: 1.962173 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.12\n",
      "================================================================================\n",
      "sumindcgners gainuch of in nine five as the rucity advarics a chince stinetwo to\n",
      "u on knot hich a seet in nowence ye volition of thundubsoms one nine nine six tw\n",
      "tred my oztvuard two lufc even zero zero zero six and bocan ono c wrisnines flud\n",
      "gwleceredive and the caly leately faeorkgrean a dueters is use manstritmentign w\n",
      "ujnine two gnon god but in moinso movensynnities of comesting chisterrostrat of \n",
      "================================================================================\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 9100: 1.955662 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.28\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 9200: 1.968265 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 9300: 1.953973 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 9400: 1.958174 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 9500: 1.951529 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 9600: 1.966000 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.12\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 9700: 1.949618 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 9800: 1.962887 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 9900: 1.966798 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 10000: 1.962664 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.63\n",
      "================================================================================\n",
      "hma calon a distrinish and the leolek divich aqlety of the derity amery the the \n",
      "rferations of evermograted of the priate in coinhhy stually was athic in inflait\n",
      "wm the the furleganing the notibned dand at rewitions bek hat thight of five a l\n",
      "icands in chaving vilitions niney one zero zero seven junt perphe g after the we\n",
      "xrals so posped enting the educting citic ins rolilis can expole and sead stacti\n",
      "================================================================================\n",
      "Validation set perplexity: 7.04\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        # transform input into IDs\n",
    "        batchesIDs = batches2IDs( batches )\n",
    "        # print(batchesIDs[0])\n",
    "        # print(batchesIDs[1])\n",
    "        # print(batchesIDs[2])\n",
    "        # print(ids2bigrams(batchesIDs[0],batchesIDs[1]))\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = ids2bigrams(batchesIDs[i], batchesIDs[i+1])\n",
    "            feed_dict[train_labels[i]] = batchesIDs[i+2]\n",
    "        feed_dict[keep_prob] = dropout_keep_prob\n",
    "        # print(feed_dict)\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed0 = sample(random_distribution())\n",
    "                    sentence = characters(feed0)[0]\n",
    "                    feed1 = sample(random_distribution())\n",
    "                    sentence += characters(feed1)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(78):\n",
    "                        # transform input into IDs\n",
    "                        # batches2IDs accepts list. we need to wrap and unwrap.\n",
    "                        bIDs = batches2IDs( [feed0, feed1] )\n",
    "                        feedID = ids2bigrams(bIDs[0], bIDs[1])\n",
    "                        prediction = sample_prediction.eval({sample_input: feedID, keep_prob: 1.0})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        feed0 = feed1\n",
    "                        feed1 = feed\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                # transform input into IDs\n",
    "                bIDs = batches2IDs(b)\n",
    "                predictions = sample_prediction.eval({sample_input: ids2bigrams(bIDs[0],bIDs[1]), keep_prob: 1.0})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
