{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise ValueError(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train is list of unrollings+1, each a np.array(64,27) of a batch of one-hot encodings\n",
      "train batch: 12 array of (64, 27)\n",
      "to-id is  list of unrollings+1, each a np.array(64) of a batch of id-embeddings\n",
      "to_id batch: 12 array of (64,)\n",
      "to bigrams [419 629 336   1 352 221 675  52 549 360 383 221 135 137 405 680 423  27\n",
      "  20  45 263   1 549 133 162  47 135 149  40 508 258 405  27 197 257 501\n",
      "  82  46   4 366 540 567 135 405 411 513 309 153 640 155 548 155  19 262\n",
      " 549 108 548 149 167 129 558  47  43 522]\n",
      "[' an']\n",
      "['nar']\n",
      "[1]\n",
      "[379]\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "def batches2IDs(batches):\n",
    "    \"\"\"Convert a sequence of batches into the ids using char2id helper.\n",
    "    \"\"\"\n",
    "    l = [ np.array( [ char2id(x) for x in characters(b) ] ) for b in batches ]\n",
    "    return l\n",
    "\n",
    "def builddictbigram():\n",
    "    d = {}\n",
    "    n = 0\n",
    "    for i in range(vocabulary_size):\n",
    "        for j in range(vocabulary_size):\n",
    "            d[(i,j)] = n\n",
    "            n+=1\n",
    "    return d\n",
    "\n",
    "dictbigram = builddictbigram()\n",
    "# print(dictbigram)\n",
    "\n",
    "def ids2bigrams(l0, l1):\n",
    "    return np.array( [ dictbigram[x0, x1] for x0, x1 in zip(l0, l1) ] )\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings+1)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1+1)\n",
    "\n",
    "bx = train_batches.next()\n",
    "bids = batches2IDs(bx)\n",
    "print('train is list of unrollings+1, each a np.array(64,27) of a batch of one-hot encodings')\n",
    "print('train batch:', len(bx), 'array of', bx[0].shape)\n",
    "print('to-id is  list of unrollings+1, each a np.array(64) of a batch of id-embeddings')\n",
    "print('to_id batch:', len(bids), 'array of', bids[0].shape)\n",
    "print('to bigrams', ids2bigrams(bids[0],bids[1]))\n",
    "\n",
    "v1 = valid_batches.next()\n",
    "v2 = valid_batches.next()\n",
    "print(batches2string(v1))\n",
    "print(batches2string(v2))\n",
    "print(ids2bigrams(batches2IDs(v1)[0],batches2IDs(v1)[1]))\n",
    "print(ids2bigrams(batches2IDs(v2)[0],batches2IDs(v2)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** PROBLEM 1 DONE IN 6_lstm **\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My Aproach:\n",
    "\n",
    "Same as 6.2b:\n",
    "\n",
    "1) Identify the observation from the label.\n",
    "\n",
    "2) Unrolling are necesary for the LSTM cells.\n",
    "\n",
    "3) Split train from labels at feed.\n",
    "\n",
    "4) Generate num_unrollings + 2 batches.\n",
    "\n",
    "5) first is bigram0, second is bigram1, third is label.\n",
    "\n",
    "6) Train over bigram2ID(bigram0, bigram1)\n",
    "\n",
    "\n",
    "Plus the dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_inputs: list of 10 (64,)\n",
      "train_labels: list of 10 (64,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size**2, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        \n",
    "        # ORIGINAL\n",
    "        # all_gates_state = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        # lookup instead of matmul. they are equivalent in this context.\n",
    "        # all_gates_state = tf.nn.embedding_lookup(ifcox, i) + tf.matmul(o, ifcom) + ifcob\n",
    "\n",
    "        ifcoxdemb = tf.nn.embedding_lookup(ifcox, i)\n",
    "        ifcoxd = tf.nn.dropout(ifcoxdemb, 1.0)\n",
    "        od = tf.nn.dropout(o, keep_prob)\n",
    "        all_gates_state = ifcoxd + tf.matmul(od, ifcom) + ifcob\n",
    "        \n",
    "        input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2*num_nodes])\n",
    "        update = all_gates_state[:, 2*num_nodes: 3*num_nodes]\n",
    "        output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data. split input/labels\n",
    "    train_inputs = list()\n",
    "    train_labels = list()\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "        train_labels.append(tf.placeholder(tf.int64, shape=[batch_size])) # ids as input.\n",
    "    print('train_inputs: list of', len(train_inputs), train_inputs[0].get_shape())\n",
    "    print('train_labels: list of', len(train_inputs), train_labels[0].get_shape())\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        # use of sparse_softmax because it accepts int64s\n",
    "        sofmacs = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        # sofmacs = tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels))\n",
    "        loss = tf.reduce_mean(sofmacs)\n",
    "\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "       1.0, global_step, 200000, 0.5, staircase=True)\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # momemtum = 0.9\n",
    "    # optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    \n",
    "    # idem as train_data\n",
    "    sample_input = tf.placeholder(tf.int64, shape = [1])    \n",
    "    # sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297495 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "tblzkogmntdloklewasorurbswemddutuvgyeacyeoiffckhpe ajywwlncd ihbrucnrxsrjznforvd\n",
      "owylcpwgkjzhabuzdnajri iklakedlrj ybfvxhakbiugs dmyilfmjaxpunaniucahklmutfalaebf\n",
      "nyphqgitxoezpqypkyytwiqbepuajfne mxgx rttmbqtzzilynbpozoxltukmnesaoxvgoswmht rpe\n",
      "kjmvjiciheubkequzdmsenujnrfymczwddikfylyu pntxlqjki igbeupewlhwkeorv thpha catru\n",
      "odicwwo xxb  qge pefrlaioeu tznt akcueersndpqpugsre ezswpolisgnzyimwydhbke otect\n",
      "================================================================================\n",
      "Validation set perplexity: 23.96\n",
      "Average loss at step 100: 2.862515 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.69\n",
      "Validation set perplexity: 16.23\n",
      "Average loss at step 200: 2.692102 learning rate: 1.000000\n",
      "Minibatch perplexity: 13.21\n",
      "Validation set perplexity: 12.43\n",
      "Average loss at step 300: 2.453883 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.96\n",
      "Validation set perplexity: 11.10\n",
      "Average loss at step 400: 2.302394 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.56\n",
      "Validation set perplexity: 9.84\n",
      "Average loss at step 500: 2.186659 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.49\n",
      "Validation set perplexity: 9.34\n",
      "Average loss at step 600: 2.122053 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.17\n",
      "Validation set perplexity: 8.88\n",
      "Average loss at step 700: 2.098512 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.07\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 800: 2.070342 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.08\n",
      "Validation set perplexity: 8.29\n",
      "Average loss at step 900: 2.051933 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.95\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 1000: 2.022337 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.49\n",
      "================================================================================\n",
      "dy rowila guann or zera for event anded id istrisby werwas of ralwas poynost the\n",
      "draley a postaloe stanist one pologessic does the of tho irnare gs fyaeat hdre p\n",
      "lrryyiva temmonl itakind elest was do it x de nimts anduraby to and sbimefetaiwy\n",
      "vigh istaint the enicalunt dar coms not collede zero binfropollhosdgan asree yeo\n",
      "gmer its st tese ne of groepare as owee moj s once worstruch jalpao baal achity \n",
      "================================================================================\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 1100: 2.030202 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 1200: 1.995956 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 1300: 1.996802 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 1400: 1.985216 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.67\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 1500: 1.970529 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 7.83\n",
      "Average loss at step 1600: 1.952816 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 1700: 1.941712 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 1800: 1.925675 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 1900: 1.925126 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 2000: 1.939166 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.30\n",
      "================================================================================\n",
      "rries decafationy alxetricialist diviouct of the zero aocirqysial foury arfislag\n",
      "oat of the eadre eavelcqin age or that of mudd in foxuallity ar tberian uwes the\n",
      "qcd fine the reeh in am sgulwids com six the stanks cilays one one nine eight ft\n",
      "pmews ace wholow fachoadai stliet catto judund the mailicial the sto and fablogi\n",
      "nne betwo the cout two zero zent shen apportal paatifficom s mistar with and ren\n",
      "================================================================================\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 2100: 1.921173 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 2200: 1.928329 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 2300: 1.916721 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 2400: 1.902327 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 2500: 1.894776 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 2600: 1.895682 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 2700: 1.910498 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 2800: 1.886426 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.87\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 2900: 1.900128 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 3000: 1.871002 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.27\n",
      "================================================================================\n",
      "glo consured by to the ling heoptionsdeufaelood a liles ato karg the the areate \n",
      "faror welle and in in land mirforge untly elishers broea untly innignoties in se\n",
      "uice untause unication othe dar afires six exels  an one ine nine five nine nine\n",
      "qpin the u ling the hosapationali hed fire molimigin for the rectuary one nine t\n",
      "ne nine nine nive two zero of cougv lime roway brhhuntriy a puwliplection retake\n",
      "================================================================================\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 3100: 1.860021 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 3200: 1.853592 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 3300: 1.870354 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.32\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 3400: 1.866385 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 3500: 1.865906 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 3600: 1.871076 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 3700: 1.870469 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 3800: 1.867208 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 3900: 1.851918 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 4000: 1.847990 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.15\n",
      "================================================================================\n",
      "wwuy nine fire in one five one nine new yeard in one nati eveling the coutlylan \n",
      "b home tment nu the a whith himplas schramete the pdainr maall almoser wasity it\n",
      "gdonne as one two a was vorwas ao smisleact of ensupaartical produte bureven sio\n",
      "xdh he operty reight zero five filmidz sou mayy do eight nattes one nine insfed \n",
      "zdigive name in isture camerch faiscontipoll is of the unisher afgicative in is \n",
      "================================================================================\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 4100: 1.852402 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 4200: 1.834460 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 4300: 1.822983 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 4400: 1.826683 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 4500: 1.828531 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 4600: 1.834545 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 4700: 1.813783 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 4800: 1.821879 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 4900: 1.820310 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 5000: 1.798983 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.29\n",
      "================================================================================\n",
      "rmy for by noverge be dirsot of the if monalite of onalral pact oration arthen o\n",
      " quxpeconalms onor as in bot guiter chor jace intited to degencoration egority l\n",
      "qrequires graw of yearthrow eviltus dirredi ceptron line of the seven it dayed w\n",
      "zqeirs nacaus to com hound taken frout thy againstiman of cresing of w threek vo\n",
      "oad a storion otho texced to bodom of traccos areelial bange goads reet of the f\n",
      "================================================================================\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 5100: 1.793889 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 5200: 1.834529 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 5300: 1.823257 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 5400: 1.814585 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 5500: 1.813955 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 5600: 1.791123 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 5700: 1.791781 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5800: 1.789334 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 5900: 1.802064 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 6000: 1.784591 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "================================================================================\n",
      "rfive leversial of kinist featundomences at the the mundsrad of the eastent two \n",
      "on centurale six by clogian of the distante his from hare to from the redage one\n",
      "cnationsbus as perica is reqming is to pation clace and the ital of theme outhe \n",
      "pcs sime gssose the sge were iid of altich brignited eventer wase usterge a mart\n",
      "zon has ducturs wrth compley three apped thought throughly tany durific spe muse\n",
      "================================================================================\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 6100: 1.788861 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 6200: 1.799703 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 6300: 1.774547 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 6400: 1.777638 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 6500: 1.772747 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 6600: 1.789110 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 6700: 1.770574 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 6800: 1.766796 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 6900: 1.773515 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 7000: 1.788658 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.10\n",
      "================================================================================\n",
      "xmwar dimiils to them like largetrolev sa workes hs and theory asse mede and of \n",
      "cv and follocilate to came and s rod a rescomentical sublist fing to the firated\n",
      "pcns iviak iref conforde your from meshad preal could gardist ge an conry ruguic\n",
      "qs a s on varation the colltati of a him there lavy it commed four the fiest of \n",
      "yqr butper year aloction and damook addhigh their in qualem the crorthressed whi\n",
      "================================================================================\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 7100: 1.768101 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 7200: 1.749316 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 7300: 1.748074 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 7400: 1.767322 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 7500: 1.730536 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 7600: 1.749169 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 7700: 1.722530 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 7800: 1.704583 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 7900: 1.714427 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 8000: 1.754199 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "================================================================================\n",
      "ek one nine sflecess from a carrregans partic dectionishing criming s leffer off\n",
      "cms prompanges elgof jsurcreatections and to the sociem to his strances oftan do\n",
      "xks guale s the aalbally his  as lonce ecton givers underm the seven the brounth\n",
      "ages later hasels nine amenchere autholes of he upor sulta angivental four feven\n",
      "ostrence to a rizonister is arroughere holaxl wars willine whin the secur one th\n",
      "================================================================================\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 8100: 1.749792 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 8200: 1.761583 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 8300: 1.743967 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 8400: 1.749822 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 8500: 1.729346 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 8600: 1.758924 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 8700: 1.738307 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 8800: 1.731303 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 8900: 1.724484 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 9000: 1.722018 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "hb verted  this atrann aly combodenx sto was as bouther albutle body camme dran \n",
      "yzaties the syste varitic a moote stite and appartity inivina firstic reathan an\n",
      "nnal the onan fervubrate contol with that ten its was uplies acpresses pronicily\n",
      "ca belient reconsident of the fidenson insuiten only one three two zero zero fou\n",
      "he hiusre and matudinda with ps has for to influage zero that aputed by who this\n",
      "================================================================================\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 9100: 1.742159 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 9200: 1.720512 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 9300: 1.722398 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 9400: 1.725876 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 9500: 1.734726 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 9600: 1.748869 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 9700: 1.733182 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 9800: 1.719426 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 9900: 1.718785 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 10000: 1.729294 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "tji in g imperinginge eloot in the eldeform fhe cest five becitions th cenry bce\n",
      "nks iumpley use the dicares live zero zero and will sape tages ac opendian ornat\n",
      "ffor wartoaried more non the delesting exim accutent signal his dated the outs p\n",
      "few and weight eight nine zero gover three five the unicuter out colso he at the\n",
      "hbacked thoughisral and which suilea scient suriped to birlys for the terral it \n",
      "================================================================================\n",
      "Validation set perplexity: 6.68\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        # transform input into IDs\n",
    "        batchesIDs = batches2IDs( batches )\n",
    "        # print(batchesIDs[0])\n",
    "        # print(batchesIDs[1])\n",
    "        # print(batchesIDs[2])\n",
    "        # print(ids2bigrams(batchesIDs[0],batchesIDs[1]))\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = ids2bigrams(batchesIDs[i], batchesIDs[i+1])\n",
    "            feed_dict[train_labels[i]] = batchesIDs[i+2]\n",
    "        feed_dict[keep_prob] = dropout_keep_prob\n",
    "        # print(feed_dict)\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed0 = sample(random_distribution())\n",
    "                    sentence = characters(feed0)[0]\n",
    "                    feed1 = sample(random_distribution())\n",
    "                    sentence += characters(feed1)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(78):\n",
    "                        # transform input into IDs\n",
    "                        # batches2IDs accepts list. we need to wrap and unwrap.\n",
    "                        bIDs = batches2IDs( [feed0, feed1] )\n",
    "                        feedID = ids2bigrams(bIDs[0], bIDs[1])\n",
    "                        prediction = sample_prediction.eval({sample_input: feedID, keep_prob: 1.0})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        feed0 = feed1\n",
    "                        feed1 = feed\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                # transform input into IDs\n",
    "                bIDs = batches2IDs(b)\n",
    "                predictions = sample_prediction.eval({sample_input: ids2bigrams(bIDs[0],bIDs[1]), keep_prob: 1.0})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
